---
title: "Linear Algebra"
format: html
execute:
  error: true
---

## Basic Matrix Operations

This is a test.

Python/numpy/scipy provide a range of options for achieving basic matrix operations. You will need to take a little care to ensure that your code implements the operations you intend it to. This is largely due to the fact that some operators/functions will change their behaviour depending on the input you provide. This section includes some recommendations for simple linear algebra, which should ensure your code behaves as desired.

Matrices can be implemented as a 2D `np.ndarray`. Basic matrix arithmetic can then be performed using standard operators `+`,`-` and `@`. You can also use `np.matmul()` for matrix multiplication. Numpy will also perform matrix multiplication with `np.dot()`, but this is not recommended if you can use `@` or `np.matmul()`.

```{python}
import numpy as np

A = np.array([[1, 2, 3,],[4, 5, 6],[1, 0, 0]])
B = np.array([[1, 0, 0,],[0, 1, 0],[0, 0, 1]])

# addition
print(A + B)

# subtraction
print(A - B)

# scalar multiplication
print(3*B)

# matrix multiplication
print(np.matmul(A, B))
print(A @ B)
```

Vectors can be implemented as 1D arrays, or as 2D arrays. A 1D array will be interpreted as row or column vector depending on the context in which it is used. Use of 2D arrays allows you to specify row or column form. This can be useful, since `np.matmul()` or `@` will throw an exception if you accidentally try to perform an illegal matrix operation.

```{python}
v  = np.array([1,2,3])
vr = np.array([[1,2,3]])
vc = np.array([[1],[2],[3]])

# two options for matrix * vector
print(A@v)
print(A@vc)

# two options for vector * matrix
print(v@A)
print(vr@A)

# this is not a valid matrix multiplication !
print(A@vr)
```

Numpy will also provide the usual forms of vector product via `np.vdot()`, `np.cross()`, `np.inner()` and `np.outer()`. Again, `np.dot()` will provide a vector dot product, but is not recommended if you can use `vdot()`.

Other useful matrix operations are provided by numpy, such as : 
- `np.transpose()` (also available via `ndarray.T`) 
- `np.norm()` 
- `np.trace()`

For further information, look at the reference pages : 
[https://numpy.org/doc/stable/reference/routines.array-manipulation.html](https://numpy.org/doc/stable/reference/routines.array-manipulation.html)
[https://numpy.org/doc/stable/reference/routines.linalg.html](https://numpy.org/doc/stable/reference/routines.linalg.html)

Finally, `scipy.linalg` provides some additional basic operations such as the determinant and the inverse.

```{python}
import numpy as np
import scipy.linalg as linalg

A = np.array([[1, 2, 3,],[4, 5, 6],[1, 0, 0]])
detA = linalg.det(A)
print(detA)

invA = linalg.inv(A)
print(invA)
```


## Simultaneous Equations

Many problems in physics require solving simultaneous equations. When these become large and complex, numerical routines are required.

A set of simultaneous equations can always be written in matrix form, for example, two equations in two unknowns ($x_1$ and $x_2$)

$$
\begin{aligned}
ax_1 + bx_2 &= y_1 \\
cx_1 + dx_2 &= y_2
\end{aligned}
$$ {#eq-a}

can be rewritten as

$$
\left(\begin{array}{cc} a & b \\ c & d\end{array}\right)
\left(\begin{array}{c} x_1 \\ x_2 \end{array}\right) = 
\left(\begin{array}{c} y_1 \\ y_2 \end{array}\right)
$$ {#eq-b}

An arbitrary set of equations is

$$Ax = y$$  {#eq-c}

where A is the matrix of coefficients, x is the vector of unknown variables $x_1$, $x_2$, ... and y is the known vector of constants.


### Inverse Matrix

One way to solve the above equation is to multiply both sides by the inverse of A:

$$A^{-1} A x = A^{-1} y$$ {#eq-d}

giving :

$$x = A^{-1} y$$  {#eq-e}

This is demonstrated in the example below for a simple test case :

$$
\left(\begin{array}{ccc} 1 & 2 & 2 \\
                        3 & 1 & 6 \\
                        0 & 2 & 2\end{array}\right)
\left(\begin{array}{c} x_1 \\ x_2 \\ x_3\end{array}\right) = 
\left(\begin{array}{c} 2 \\ 7 \\ 1\end{array}\right)
$$  {#eq-f}

```{python}
import numpy as np
import scipy.linalg as linalg

def solve_inv(a,y):
    x = linalg.inv(a) @ y
    return x

a = np.array([[1, 2, 2,],[3, 1, 6],[0, 2, 2]])
y = np.array([[2], [7], [1]])
print(a)
print(y)

x = solve_inv(a,y)
print(x)
```

Is this the solution? We can easily check by inserting the solution into the original equation.

```{python}
print(a @ x)
```

Which is indeed equal to our `y` above. This kind of test is known as a 'closure test' and will be used frequently throughout this unit to verify our code.

Before using this method for solving simultaneous equations, though, we should understand how `scipy.linalg.inv` finds the matrix inverse.  Unfortunately, this is tricky to understand from the reference page, ([scipy.linalg.inv](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.inv.html)). But, if you examine the source code for this function you'll see that it uses a function called DGETRI. This is defined in the LAPACK library, and its reference page is [here](https://www.netlib.org/lapack/explore-html-3.6.1/dd/d9a/group__double_g_ecomputational_ga56d9c860ce4ce42ded7f914fdb0683ff.html).  As you can see this routine uses LU decomposition to find the inverse! It doesn't make sense, therefore, to find a matrix inverse simply to solve a simultaneous equation, and using LU decomposition directly will involve fewer operations.  However, there are exceptions when dealing with many simultaneous equations. For example, suppose you have a sequence of problems which all feature the same matrix $A$, but have different RHS $y$. In this case it would be efficient to invert $A$ once, then multiply by $y$ to solve each problem, since multiplication involves fewer operations than LU decomposition.

As an aside, LAPACK is a linear algebra library written in FORTRAN - which remains one of the most efficient languages for writing numerical methods - and most routines in `scipy.linalg` basically provide a Python interface to this library.


### Gaussian Elimination

Some sets of simultaneous equations are easy to solve.  For example :

$$
\begin{aligned}
a x_1 &= y_1 \\
b x_2 &= y_2 \\
c x_3 &= y_3
\end{aligned}
$$  {#eq-g}
This can be written in what is known as row echelon form :

$$
\left(\begin{array}{ccc} a & 0 & 0 \\
                        0 & b & 0 \\
                        0 & 0 & c\end{array}\right)
\left(\begin{array}{c} x_1 \\ x_2 \\ x_3\end{array}\right) = 
\left(\begin{array}{c} y_1 \\ y_2 \\ y_3\end{array}\right)
$$  {#eq-h}

And then reduced row echelon form :

$$
\left(\begin{array}{ccc} 1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1\end{array}\right)
\left(\begin{array}{c} x_1 \\ x_2 \\ x_3\end{array}\right) = 
\left(\begin{array}{c} y_1/a \\ y_2/b \\ y_3/c\end{array}\right)
$$  {#eq-i}

Gauss-Jordan elimination is a process which reduces any linear equation set to this form. It can be shown that the reduced row echelon form is unique, and therefore independent of the order of operations which are used to find it. The technique is illustrated using the example problem from earlier. It's convenient to use the 'augmented' matrix, which includes the right-hand side.
$$
\left(\begin{array}{ccc|c} 
    1 & 2 & 2 & 2 \\
    3 & 1 & 6 & 7 \\
    0 & 2 & 2 & 1
\end{array}\right)
$$  {#eq-j}

Then we apply simple operations until we obtain the equation in row echelon form. These operations include:

- Multiply a row by a constant
- Swap two rows
- Sum two rows in a linear combination

(Hopefully, these sound reasonably familiar - we are just formalising techniques you will have used before)

Replace $R_1$ (row 1) with $R_1 - R_3$ :
$$
\left(\begin{array}{ccc|c} 
    1 & 0 & 0 & 1 \\
    3 & 1 & 6 & 7 \\
    0 & 2 & 2 & 1
\end{array}\right)
$$  {#eq-k}

Replace $R_2$ with $R_2 - 3R_1$ :
$$
\left(\begin{array}{ccc|c} 
    1 & 0 & 0 & 1 \\
    0 & 1 & 6 & 4 \\
    0 & 2 & 2 & 1
\end{array}\right)
$$  {#eq-l}

Replace $R_2$ with $R_2 - \frac{2}{5}R_2$ :
$$
\left(\begin{array}{ccc|c} 
    1 & 0 & 0 & 1 \\
    0 & -5 & 0 & 1 \\
    0 & 0 & 2 & \frac{7}{5}
\end{array}\right)
$$  {#eq-m}

And then finally for reduced row echelon form, replace $R_2$ with $\frac{-1}{5}R_2$ and $R_3$ with $\frac{1}{2}R_3$
$$
\left(\begin{array}{ccc|c} 
    1 & 0 & 0 & 1 \\
    0 & 1 & 0 & -\frac{1}{5} \\
    0 & 0 & 1 & \frac{7}{10}
\end{array}\right)
$$  {#eq-n}

So the solution is :
$$
\begin{aligned}
x_1 &= 1 \\
x_2 &= -\frac{1}{5} \\
x_3 &= \frac{7}{10}
\end{aligned}
$$  {#eq-o}

### LU Decomposition

Matrix decomposition techniques involve factorising a general matrix into a product of several matrices. LU decomposition involves writing the general matrix, $A$, as the product of two triangular matrices, $L$ and $U$.

$$
A=
\left(\begin{array}{ccc} a_{11} & a_{12} & a_{13} \\
                         a_{21} & a_{22} & a_{23} \\
                         a_{31} & a_{32} & a_{33}
\end{array}\right)
$$  {#eq-p}

$$
A=LU=
\left(\begin{array}{ccc} 1      & 0      & 0 \\
                         l_{21} & 1      & 0 \\
                         l_{31} & l_{32} & 1
\end{array}\right)
\left(\begin{array}{ccc} u_{11} & u_{12} & u_{13} \\
                         0      & u_{22} & u_{23} \\
                         0      & 0      & u_{33}
\end{array}\right)
$$  {#eq-q}

We can use LU decomposition to solve matrix equations since it allows us to write the equation $$Ax = y$$ as $L(Ux)=y$.  This can then be written as two equations $Lc=y$ and $Ux=c$, which are trivially solved, first for $c$, and then for $x$.

The matrices $L$ and $U$ can be found using the operations described above for Gaussian elimination. There are several algorithmic formulations that define the sequence of operations. Scipy provides an LU decomposition routine: `scipy.linalg.lu()`. Note that this performs a variation on the LU decomposition described above, since it also computes a permutation matrix $P$, such that 

$$PA = LU$$  {#eq-r}

Scipy also provides a simple function to obtain the solutions to a matrix equation. `scipy.linalg.lu_solve()` expects the $L$, $U$ and $P$ matrices as arguments, as shown in the example below.

- [scipy.linalg.lu](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu.html)

- [scipy.linalg.lu_solve](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu_solve.html)

```{python}
def solve_lu(a,y):
    lu, piv = linalg.lu_factor(a)
    x = linalg.lu_solve((lu, piv), y)
    return x

print(solve_lu(a,y))
```

Note that the general purpose solvers provided by both numpy and scipy both utilise LU decomposition :

- [numpy.linalg.solve](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html)

- [scipy.linalg.solve](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html)


### SVD Decomposition

LU decomposition will find an exact solution to the matrix equation in a wide variety of cases.  However, a solution may not exist, or there may be infinite solutions. In such cases, the Singular Value Decomposition may be of use.

For an $m \times n$ matrix $A$, the singular values, $\sigma$ are given by the solutions to

$$
\begin{aligned}
Av &= \sigma u \\
A^Tu &=\sigma v
\end{aligned}
$$  {#eq-s}

where $u$ and $v$ are two non-zero vectors.  These equations are closely related to the eigenvalue equation.  Indeed, the singular values are also the square roots of the eigenvalues of $A^TA$.

The singular value decomposition of $A$ is

$$A = U\Sigma V^T$$  {#eq-t}

where $U$ and $V$ are orthonormal matrices, and $\Sigma$ is a matrix with the singular values on its leading diagonal, and zero elsewhere.

The SVD decomposition allows use to compute the pseudo-inverse of $A$, which is given by :

$$A^\dagger = V \Sigma^\dagger U^T$$  {#eq-u}

where $\Sigma^\dagger$ is the pseudo-inverse of $\Sigma$ and is obtained by transposing $\Sigma$ and replacing each non-zero element with it's reciprocal.

The pseudoinverse (also known as the Moore-Penrose inverse) can always be computed, even when $A$ is singular, ie. when when $\frac{1}{|A|}=0$ and the inverse cannot be found.

In the context of solving a matrix equation $Ax=y$, the product of pseudoinverse and the RHS, (i.e. $\bar{X}=A^\dagger y$) has various properties. When A is non-singular, $\bar{x}$ gives the solution to $Ax=y$.  When $A$ is singular, $\bar{x}$ is a least squares approximation to the nearest solution.  When $Ax=y$ has a space of solutions (equivalent to a set of simultaneous equations with degeneracy), then $\bar{x}$ is a vector which describes this space.

SVD decomposition is available in Scipy using `scipy.linalg.svd()`. For further information, see [scipy.linalg.svd](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html). Note that, unlike LU decomposition, no `solve()` function is supplied, and instead we must write some code to calculate $\bar{x}$.

```{python}
def solve_svd(a,y):
    u, s, v = linalg.svd(a)
    x = v.T @ np.diag(1/s) @ u.T @ y
    return x

print(solve_svd(a,y))
```

Alternatively, a function to compute the pseudoinverse directly is provided, [scipy.linalg.pinv](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.pinv.html).

```{python}
print( linalg.pinv(a) @ y)
```

### Physics Example

Here we illustrate the use of simultaneous equation solvers in a familiar context - the use of Kirchoff's laws and Ohm's law to understand resistor networks. Typically, analysis of a resistor network will involve solving simultaneous equations, to calculate voltage and current at the desired points in the network. Consider the electronic circuit shown in the diagram.

![Figure 1 - Example resistor network.](circuit.png)

Where : $V_1 = 12V$, $V_2 = 12V$, $R_1 = 3 \Omega$, $R_2 = 3 \Omega$, $R_3 = 10 \Omega$, $R_4 = 2 \Omega$, $R_5 = 2 \Omega$.

By identifying the three current loops indicated, we can use Kirchoff's loop rule and Ohm's law to write : 
$$
\begin{aligned}
V_1 &= I_1 R_1 + (I_1 - I_2) R_2 \\
0   &= (I_2 - I_1) R_2 + I_2 R_3 + (I_2 - I_3) R_4 \\
-V_2 &= (I_3 - I_2) R_4 + I_3 R_5
\end{aligned}
$$  {#eq-v}

So we have a set of simultaneous equations, which we can write as a matrix equation :

$$
\begin{pmatrix}
R_1+R_2 & -R_2 & 0 \\
-R_2 & R_2+R_3+R_4 & -R_4 \\
0  & -R_4 & R_4+R_5 \\
\end{pmatrix}
\begin{pmatrix}
I_1 \\
I_2 \\
I_3
\end{pmatrix}
=
\begin{pmatrix}
V_1 \\
0 \\
-V_2
\end{pmatrix}
$$  {#eq-w}

Solving this matrix equation will provide the current at all points in the circuit. This method is known as "mesh analysis" of circuits.

We can write a simple function that, given the voltage and resistor values, will return the currents :

```{python}
def meshAnalysis(v1, v2, r1, r2, r3, r4, r5):
    m = np.array ([[r1+r2, -r2, 0],[-r2,r2+r3+r4,-r4],[0,-r4,r4+r5]])
    v = np.array([[v1],[0],[-v2]])
    i = linalg.solve(m,v)
    return i
```

Which, for the values given, will return the three currents :

```{python}
i = meshAnalysis(12, 12, 3, 3, 10, 2, 2)

print(i)
```

Or we could calculate, for example, how $I_2$ will vary as a function of $R_4$, with all other values fixed :

```{python}
r4s = np.linspace(0.1, 5.0, 100)
i2s = np.empty(len(r4s))

for j,r4 in enumerate(r4s):
    i     = meshAnalysis(12, 12, 3, 3, 10, r4, 2)
    i2s[j] = i[1][0]

import matplotlib.pyplot as plt

plt.plot(r4s, i2s)
plt.ylabel('$I_2$ (A)')
plt.xlabel('$R_4$ (ohm)')
plt.show()
```

## Eigenproblems

A square NxN matrix $A$, has eigenvector $u$ and eigenvalue $\lambda$ that satisfy :

$$(A - \lambda I)u = 0$$  {#eq-a}

Details of the numerical methods used for solving eigenproblems are beyond the scope of this course, but they are usually iterative methods, similar to those described in [@sec-relax].

It's also worth noting that solving eigenproblems is closely related to finding the roots of polynomials. You may be familiar with one method for finding eigenvalues, which is to find the roots of the N-th degree polynomial found by expanding :

$$p(t) = \det{|A - t I|} = 0$$ {#eq-b}

Many eigenproblem solving routines are provided by SciPy. In particular, `scipy.linalg.eig(A)` will return a tuple containing the eigenvalues and eigenvectors of A.  If only the eigenvalues are required, `scipy.linalg.eigenvals(A)` can be used.

Care should be taken when using `scipy.linalg.eig`, since it will find "left" and "right" eigenvectors, as specified, which are the solutions to $v A = \lambda v$ and $A v = \lambda v$ respectively.

For further details see [scipy.linalg.eig](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eig.html)

Additional routines include :

- [scipy.linalg.eigh](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html) (for Hermitian matrices)

- [scipy.linalg.eig_banded](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eig_banded.html) (for banded matrices)

- [scipy.sparse.linalg.eigs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigs.html) (for sparse, square symmetric matrices)

### Simple example

We can test these sovlers using the matrix :

$$A = 
\pmatrix{
-2 & -4 & 2 \\
-2 &  1 & 2 \\
4  &  2 & 5}
$$  {#eq-c}

for which the eigenvalues are $\lambda^{(0)}=6$, $\lambda^{(1)}=-5$, $\lambda^{(2)}=3$.

Note that the algorithms discussed here will all find _unit_ eigenvectors.  The eigenvector corresponding to $\lambda^{(0)}$ is then :

$$\hat{u}^{(0)}=\pmatrix{\frac{1}{\sqrt{293}} \\
\frac{6}{\sqrt{293}} \\
\frac{16}{\sqrt{293}}
}
=
\pmatrix{0.058 \\
0.351 \\
0.935}$$  {#eq-d}

with numerical values given to 3 decimal places on the RHS.

```{python}
import numpy as np
import scipy.linalg as linalg

m = np.array([[-2,-4,2],[-2,1,2],[4,2,5]])

# set seed for repeatability
np.random.seed(2)

# run the algorithm
mus, vs = linalg.eig(m)

# print results
np.set_printoptions(precision=3)
for i in range(3):
    print("Eigenvalue/vector : {:.1f} {}".format(mus[i], vs.T[i]))
```

Which includes the solution expected.

Note that we have transposed the array of eigenvectors returned by `linalg.eig()`. This is a feature of the function, as described in the reference manual : _"The normalized left eigenvector corresponding to the eigenvalue `w[i]` is the column `vl[:,i]`"_.

### Physics Example

In this section, we illustrate the use of eigenvalue solvers in finding stable solutions of the coupled system of oscillators shown below.

![Figure 2 - Coupled oscillators, masses on springs.](masses.png)

If the displacement of the $i$th mass from its equilibrium position is denoted as $x_i$, the force on the mass is given by the tension in the two springs as :

$$F_i = −k(x_i − x_{i−1}) + k(x_{i+1} − x_i) = −k(2x_i − x_{i−1} − x_{i+1})$$  {#eq-e}

We can assume that there are normal mode solutions, i.e. solutions of the form $x_i = z_i e^{i\omega t}$ in which all masses oscillate with the same frequency $\omega$ but with unknown phasors $z_i$. Then the above equation becomes :

$$F_i = m\ddot{x}_i = −m\omega^2x_i = −k(2x_i − x_{i−1} − x_{i+1})$$  {#eq-f}

This is one row of a matrix equation describing the entire system :

$$m\omega^2x_i \left(\begin{array}{c} \vdots \\ \\ x_i \\ \\ \vdots \end{array}\right) = 
\left(\begin{array}{ccccccc} & & & \vdots & & & \\ \cdots & 0 & -1 & 2 & -1 & 0 & \cdots \\ & & & \vdots & & & \end{array}\right)
\left(\begin{array}{c} \vdots \\ x_{i-1} \\ x_i \\ x_{i+1} \\ \vdots \end{array}\right)
$$  {#eq-g}

This example is a typical eigenvalue problem, in that many of the matrix elements are zero, which can greatly simplify the computational challenge and make even large systems solvable. The matrix is symmetric, which means it is suitable for solving with our eigenproblem solving function above, or one of the solvers from `scipy.linalg`.

```{python}
m = np.array([[2, -1,  0,  0,  0,  0,  0],
              [-1, 2, -1,  0,  0,  0,  0],
              [0, -1,  2, -1,  0,  0,  0],
              [0,  0, -1,  2, -1,  0,  0],
              [0,  0,  0, -1,  2, -1,  0],
              [0,  0,  0,  0, -1,  2, -1],
              [0,  0,  0,  0,  0, -1,  2]])

mus, vs = linalg.eig(m)
```

The eigenvalue associated with each mode gives the frequency, while the (complex) eigenvector provides the magnitude and phase of oscillation for each mass. We can plot the displacement of each mass as a function of time for each mode.

```{python}
import numpy as np
import scipy.linalg as linalg
import matplotlib.pyplot as plt

# a function to calculate the (real) displacement from complex phase
def disp(zi, omega, t):
    return np.real(zi * np.exp(1j * omega * t))

# set up some pretty colours for plotting
cm  = plt.cm.viridis
col = [cm(int(x*cm.N/7)) for x in range(7)]

# time period
ts = np.arange(0,40, 0.001)

# loop over eigenmodes
for i in range(7):
    
    print("Mode       : ",i)
    print("Eigenvalue : ", mus[i])

    fig=plt.figure(figsize=(16, 4))

    xs = []
    
    # loop over masses
    for j in range(7):
        
        # get the displacement, and add an offset to separate out each line
        offset = (2*j)-6
        
        # create displacement values from function using eigenvectors and eigenvalues
        xs     = disp(vs.T[i][j], mus[i], ts) + offset
        
        # plot displacement
        plt.plot(ts, xs, color=col[j])
        
        # plot central position to guide the eye
        plt.plot([0, 40], [offset, offset], color=col[j], linestyle='dotted') 

    plt.xlabel("t")
    plt.show()
```


