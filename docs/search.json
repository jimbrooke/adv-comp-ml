[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Computational Physics & Machine Learning",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Pre-requisites\nThese notes assume you have studied computational physics up to 2nd year (Level 5), such as PHYS20035 (Computational Physics and Data Science) or SCIF20002 (Programming and Data Analysis for Scientists).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "adv-python.html",
    "href": "adv-python.html",
    "title": "2  Advanced Python Programming",
    "section": "",
    "text": "2.1 Object Oriented Programming\nObject oriented programming is a programming paradigm that organises code around objects rather than functions and logic. Here we introduce some of the fundamental concepts (which you may have encountered before) including classes and objects. The four core principles of OO programming are then introduced, followed by some practical tips.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Advanced Python Programming</span>"
    ]
  },
  {
    "objectID": "adv-python.html#object-oriented-programming",
    "href": "adv-python.html#object-oriented-programming",
    "title": "2  Advanced Python Programming",
    "section": "",
    "text": "2.1.1 Classes & Objects\nUnsurprisingly, the fundamental building block of OO programming is the ‘object’. An object can be thought of as an entity which encapsulates some data and some functions. The data associated with an object are often known as its ‘attributes’, while the functions are often called its ‘methods’. An object can be assigned to a variable, which can then be used to access the data and call the functions.\nA class is a definition for a type of object. It will describe the attributes and methods that an object of this class must contain. Multiple objects of the same class can be generated in a program. Typically, these objects will contain different data. The method definitions are usually common to all objects of a given class, but are always called in the context of a particular object, whose data they will likely act upon.\nTo illustrate these, we will define a class that describes a 3D vector. Note that this is purely an example - if you need a representation of 3D vectors in practise, there are already many good ones available, eg. numpy.ndarray\n\nimport math\n\nclass Vector():\n    n_dimensions = 3\n    def __init__(self, x=0., y=0., z=0.):\n        self._x = x\n        self._y = y\n        self._z = z\n    \n    def __str__(self):\n        return \"x:{0} y:{1} z:{2} norm:{3}\".format(self._x, self._y, self._z, self.magnitude())\n    \n    def x(self):\n        return self._x\n\n    def y(self):\n        return self._y\n    \n    def z(self):\n        return self._z\n\n    def set_x(self, x):\n        self._x = x\n        return\n\n    def set_y(self, y):\n        self._y = y\n        return\n    \n    def set_z(self, z):\n        self._z = z\n        return\n\n    def magnitude(self):\n        return math.sqrt(self._x**2 + self._y**2 + self._z**2)\n\nThe class defines an attribute n_dimensions which will be the same for all objects of this type, and is equal to 3.\nThe __init__() method is as special method, sometimes known as the constructor. If it is defined, it will be called when an object is created. We can use this to create and initialise attributes which are unique to the object, here the three components, _x, _y, and _z. These have a leading underscore to indicate the data is internal to the object.\nThe x(), y(), z() methods provide access to the internal data values. The set_x(), set_y(), set_z() methods allow these attributes to be updated. (See Section 2.1.3 for details on why this is needed).\nThe __str__() method is a useful method which Python will call if you try to convert the object to a string type, eg. by calling print() with the object as an argument, as in the example below. Again, the\nAgain, the double-underscore convention is used to indicate that methods are intended for internal use only, and not by the end user of the class. The two methods here are used by other Python code, but you can define your own internal methods if you find it useful.\nThe magnitude() method does what it says on the tin and returns the magnitude of the vector. This method is intended for the end-user, so it doesn’t include the double-underscores.\nNote the self keyword as an argument to all methods. This is used within a method definition to refer to the particular object upon which the method is being called. Eg. when we call v.magnitude() in the example below, self is equal to v.\nNow we can create some vectors and access their components.\n\n# use the constructor to create a vector and set its components\nv = Vector(3., 4., 0.)\nprint(v)\n\n# now change one of the components\nv.set_z(5.)\nprint(v)\n\nprint(v.magnitude())\n\nx:3.0 y:4.0 z:0.0 norm:5.0\nx:3.0 y:4.0 z:5.0 norm:7.0710678118654755\n7.0710678118654755\n\n\n\n\n2.1.2 Operator overloading\nPython defines a large number of “internal” functions, some of which correspond to operators, such as +, -, /, *. For example, the operation a + a result in a call to the __add__() function. We can redefine these functions to change how these operators behave when operating on a particular class. This is known as operator overloading.\nIn the context of our Vector class, we can encode vector algebra in the class methods. In the code below, note that we pass TWO arguments to each of the operator methods add(), sub(), mul(). The first, self, is the LHS of the operator, the second, a, is the RHS. Also note how we create and return a new Vector object for the + and - methods.\n\nclass Vector():\n    n_dimensions = 3\n    def __init__(self, x=0., y=0., z=0.):\n        self._x = x\n        self._y = y\n        self._z = z\n    \n    def __str__(self):\n        return \"x:{0} y:{1} z:{2} norm:{3}\".format(self._x, self._y, self._z, self.magnitude())\n    \n    def x(self):\n        return self._x\n\n    def y(self):\n        return self._y\n    \n    def z(self):\n        return self._z\n\n    def magnitude(self):\n        return math.sqrt(self._x**2 + self._y**2 + self._z**2)\n        \n    def __add__(self, a):\n        return Vector(self._x+a._x, self._y+a._y, self._z+a._z)\n    def __sub__(self, a):\n        return Vector(self._x-a._x, self._y-a._y, self._z-a._z)\n    def __mul__(self, a):\n        return self._x*a._x + self._y*a._y + self._z*a._z\n\nClearly we have made a choice to define * as the dot product. We could have defined it as the cross product.\n\nv = Vector(4., 3., 0.)\nu = Vector(1., 1., 1.)\nprint(u+v)\nprint(u-v)\nprint(u*v)\n\nx:5.0 y:4.0 z:1.0 norm:6.48074069840786\nx:-3.0 y:-2.0 z:1.0 norm:3.7416573867739413\n7.0\n\n\n\n\n2.1.3 Encapsulation\nEncapsulation regards the bundling of data and methods into a class. Typically, good encapsulation will also involve restricting access to some of the data/methods in the class. This ‘data hiding’ prevents the outside world from directly operating on, or modifying, the class data. The only way to interact with an object of the class will be via its methods. This has several benefits :\n\nCode re-use. The class is designed for a well-defined purpose, and can be easily re-used when required, just by creating a new object of that class.\nSimplifying external code. Users of the class don’t need to know about any complexities of its internal implementation.\nIncreased maintainability. Since the interface to the class is fixed by its methods, the internal operations of the class can be modified without affecting users of the class.\nSecurity. Modification of the class data via unauthorized or unintended methods is prevented.\n\nThese benefits are only realised when classes are well designed. The role of the class, and its interfaces (ie. the class methods), need to be clearly defined, and a developer will need to have understood all the realistic ways the class will be used. (Which we sometimes call the “use cases”).\nIn our example, encapsulation is provided by accessing the internal data of the Vector class (ie. the _x, _y, _z) members via the methods (ie. x(), y(), z()). Note that Python does not provide a way to make data private. The leading underscore convention is an indication to the user that these data should not be directly accessed. But there is no way to enforce it. Other languages, such as C++, do provide mechanisms enforce privacy and will produce errors if access of private data is attempted.\n\n\n2.1.4 Inheritance\nInheritance is a mechanism of OO programming whereby a class can be derived from another class, “inheriting” its properties. Here we’ll talk about a child class inheriting attributes and methods from a parent class. Usually, the child class will have some additional attributes or methods that do not belong to the parent class.\nTo illustrate this, we could consider extending the Vector class to represent a force acting on a point, which might be a useful class in describing problems in mechanics. The force vector would be inherited from Vector, and the point would be an additional Vector member.\n\nclass ForceOnPoint(Vector):\n    def __init__(self, x=0., y=0., z=0., rx=0., ry=0., rz=0.):\n        Vector.__init__(self,x, y, z)\n        self._r = Vector(rx, ry, rz)\n\n    def rx(self):\n        return _r.x()\n\n    def ry(self):\n        return _r.y()\n\n    def rz(self):\n        return _r.z()\n\n    def set_rx(self, rx):\n        _r.x = rx\n        return\n\n    def set_ry(self, ry:\n        _r.y = ry\n        return\n\n    def set_rz(self, rz):\n        _r.z = rz\n        return\n\nSyntaxError: '(' was never closed (3896604152.py, line 19)\n\n\nNote how the ForceOnPoint defines a new constructor, which calls the Vector constructor and then an additional line to add the new attribute (the point). We have provided methods to access components of the point vector, which themselves use the component access methods of Vector. So if we wanted to change the internal representation of Vector later, we could do that.\nNote that we’ve used the Vector class to define the r attribute that represents the point. This inclusion of a class within a class is known as “composition”. This is an alternative method to inheritance that will incorporate a class within a class. We could extend this to define the ForceOnPoint class using composition and not inheritance :\n\nclass ForceOnPoint(object):\n    def __init__(self, x=0., y=0., z=0., rx=0., ry=0., rz=0.):\n        # the force vector\n        self._f = Vector(x, y, z)\n        # the point vector\n        self._r = Vector(rx, ry, rz)\n\nHere, the new class represents the force and the point using two separate Vector attributes. We would need to define all the access methods for components of the force and the point.\n\n\n2.1.5 Abstraction\nAbstraction refers to defining the interface to a type of object as a set of methods. It is related to encapsulation, since it is another way of hiding internal details or complexity. To fully exploit this concept, we can define a so-called “abstract base class” which contains only method definitions (not implementation) and no attributes. The abstract class cannot be instantiated as an object, because it has no attributes and no methods that can be called - it is simply an interface definition. Sometimes it is referred to as a blueprint for real classes. A child class which inherits from this class must provide implementations for the methods, together with any required attributes. Sometimes the child class is called a “concrete class”, as it can be instantiated.\nTypically, abstract base classes are used when we have a collection of related classes, which we want to have some common features. This is illustrated with a simple example of an abstract base class that describes a vehicle, with several concrete classes that inherit from it.\n\nclass Vehicle(object):\n    def n_wheels(self):\n        pass\n    def motorised(self):\n        pass\n    def mass(self):\n        pass\n\nclass Car(Vehicle):\n    def __init__(self, m):\n        self._m = m\n    def n_wheels(self):\n        return 4\n    def motorised(self):\n        return True\n    def mass(self):\n        return _m\n\nclass Bicycle(Vehicle):\n    def __init__(self, m):\n        self._m = m\n    def n_wheels(self):\n        return 2\n    def motorised(self):\n        return False\n    def mass(self):\n        return _m\n\nNote the use of the pass keyword in the abstract base class to complete the method definition without providing any functionality.\n(Note that Python does not support ‘true’ abstract base classes natively. There are several reasons the Vehicle class in the example above is not truly abstract, which are beyond the scope of this unit. A simple reason is that you can create an instance of the Vehicle class without errors. For the purists, true abstraction is supported via the abc module).\n\n\n2.1.6 Polymorphism\nPolymorphism means “many forms”, and refers to an ability to treat objects of many different types as if they are of a single common type. In OO programming, this is usually achieved through inheritance and abstraction. For example, in the previous example, we can treat objects of type Car and Bicycle as if they are both just of type Vehicle. If we’re only interested in computing the mass of the Vehicles in the list, it doesn’t matter that some are Cars and some are Bicycles - we just call the mass() method on each one.\nPython supports polymorphism even without use of OO/inheritance/abstraction. The Python interpreter will attempt to call a method on object; if the method exists then it will run, and if it doesn’t an error will be generated. So it doesn’t matter if two objects have the same method via inheritance, or simply because we gave them both a method with the same name. This is known as “duck typing” - if an object quacks like a duck, then Python will treat it as a duck. (Python will also treat the same object as a horse, provided it finds the methods expected of a horse).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Advanced Python Programming</span>"
    ]
  },
  {
    "objectID": "adv-python.html#functional-programming-lambdas",
    "href": "adv-python.html#functional-programming-lambdas",
    "title": "2  Advanced Python Programming",
    "section": "2.2 Functional Programming & Lambdas",
    "text": "2.2 Functional Programming & Lambdas\nFunctional programming is another programming paradigm. In some respects it is quite the opposite of OO programming. Objects intrinsically have some ‘state’ associated with them, due to their attributes (internal data). So the result of a function call in OO programming (ie. a method) will depend on both the arguments passed to the function, and the state of the object upon which the method is called.\nConversely, in functional programming, computation is expressed in terms of functions which have no state. This paradigm is based on a formal system called λ-calculus, which has been shown to be Turing-complete, ie. it can simulate any Turing machine, or it can implement any computer algorithm. There are several languages (eg. Haskell, Erlang) which are designed to enforce functional programming. It is possible to write fully functional programs in Python, using lambda functions.\nDeveloping full functional programs is beyond the scope of this unit, but it should be noted that functional programming can be extremely useful in distributed computing. Because functions have no state, all the data they need to operate is passed as an argument. This means it doesn’t really matter where the function runs. Provided there is a network connection to the computer which is running the function, the arguments to the function call and the returned value can be communicated. Conversely, in OO programming, methods need to be run in the same location as the obect attributes they are associated with. This means OO programs can be harder to run in a distributed fashion than functional ones.\nA full introduction to functional programming is beyond the scope of the unit, but below we introduce the syntax for lambda functions, and some particular cases where they can be useful.\n\n2.2.1 Lambda Functions\nIn Python, lambda functions are created using the lambda keyword, eg.: lambda: x, x**2 returns an “anonymous” function that takes one argument, and returns a value that depends only on the argument (in this case, the square). Compare this to regular functions, which are declared with def and are named :\n\ndef function(x):\n    return 0.5*x**2 + x + 1\nfunction(4)\n\n13.0\n\n\nThe equivalent using a lambda function would be :\n\n(lambda x: 0.5*x**2 + x + 1)(4)\n\n13.0\n\n\n\n\n2.2.2 Anonymous functions\nLambdas can be useful when implementing mathematical expressions, and when working with functions that expect other functions as arguments. Eg. suppose I want to calculate \\(\\int_1^4 e^{−x}\\). I can do this in one line with a lambda function :\n\nimport scipy.integrate\nimport math\n\nscipy.integrate.quad(lambda x: math.exp(-1*x), 1., 4.)\n\n(0.3495638022827081, 3.880937818697784e-15)\n\n\n\n\n2.2.3 Map-Reduce with Lists\nAnother area where lambda functions are useful is in the “map-reduce” paradigm. The idea here is that when processing large amounts of data, you want to do as much processing in parallel as possible. A given algorithm is divided into a parallel part (map), and a non-parallel part (reduce). When processing huge datasets, the map will be running in parallel on multiple machines. In Python, the map() function takes a function and a list as arguments. The function is applied to each element in the list, and then a list of the results is returned. Eg. to calculate the expression below for for n = 1000 : \\[\n\\sum_{i=1}^n i^2\n\\]\n\nimport numpy as np\nfrom functools import reduce\n\nx = np.arange(0,1000)\n\nsquares = list(map((lambda x: x**2), x))\nsum = reduce((lambda x, y: x+y), squares)\n\nprint(sum)\n\n332833500\n\n\nAnother useful function when working with lists is filter(). This takes a function and a list, applies the function to each element, and retains that element if the function returns True. For example, you can try writing a function that accepts the odd values in a list of integers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Advanced Python Programming</span>"
    ]
  },
  {
    "objectID": "adv-python.html#exceptions",
    "href": "adv-python.html#exceptions",
    "title": "2  Advanced Python Programming",
    "section": "2.3 Exceptions",
    "text": "2.3 Exceptions\nThis section is a short introduction to “Exceptions”, an error handling technique commonly used in modern programming languages. It is the standard way to deal with errors in Python and both numpy and scipy use them. Raising exceptions in your own functions, and knowing how to deal with exceptions raised by other code, will help you write more robust code. It’s worth noting that exceptions are not the only way of handling errors. Code using the simple method below will be entirely adequate in many cases. You will need to decide what is appropriate for your particular case. However, understanding exceptions is important when using libbraries (such as SciPy) which raise exceptions when they encounter errors. First we’ll look at error handling in a simple case, without using exceptions.\n\n2.3.1 Simple Error Handling\nSuppose I have a function that works for positive number, but I know it will fail if given a negative input. Rather than let the code crash, or (worse) return incorrect results, we can detect this problem and do something sensible. One option might be to test the argument given to the function, print an error message, and return a default value (NaN, in this case) :\n\nimport math\nimport numpy as np\ndef mySqrt(x):\n    if x&lt;0:\n        print(\"Input must be positive.\")\n        return np.nan\n    return math.sqrt(x)\n\nLet’s test this with a couple of examples :\n\nprint(mySqrt(4))\nprint(mySqrt(-3))\n\n2.0\nInput must be positive.\nnan\n\n\nThis method for dealing with errors is better than nothing, but it has limitations. In particular : 1. Returning a default value when the input is invalid may cause further knock-on problems 2. We have no way of knowing what caused our function to be called with an invalid argument\nWe can avoid 1. by eg. halting execution of the program after printing the error message, eg. by calling sys.exit(). However, this is quite extreme and maybe not appropriate for all cases. And even if halting execution is the only option - we still have no way of dealing with point 2.\n\n\n2.3.2 Error Handling with Exceptions\nExceptions are the standard way to deal with errors in Python, and both numpy and scipy use them. Raising exceptions in your own functions, and knowing how to deal with exceptions raised by other code, can help you write more robust code. However, exceptions are just an error han- dling technique - you will still need to analyse where errors can occur in your code and make suitable provisions for handling them. The key feature of exceptions is that they allow the programmer to decide where in the code is the appropriate place to take action. In the example here, this might be in the function that calls mySqrt(). But in other circumstances, it might be in the function that calls that function. When an exception is generated, it is communciated back through all function calls until a block of code ‘catches’ the exception(and takes some action. Ultimately, if nothing handles the exception, the program will stop. Here is how we would handle this error using an exception :\n\ndef mySqrt(x):\n    if x&lt;0:\n        raise Exception(\"Negative input\")\n    return math.sqrt(x)\n\nprint(mySqrt(4))\nprint(mySqrt(-4))\n\n2.0\n\n\nException: Negative input\n\n\nYou might have seen this kind of print out when debugging code. The “Traceback” lists the function calls that led to the exception being raised.\n\n\n2.3.3 Catching Exceptions\nRaising an exception is only half of the process. The other half is “catching” them. Let’s say we have a function that calls mySqrt() but it knows what to do if the exception is raised. We can use a “try-except” (also known as “try-catch”, from the corresponding C++ keywords) block to catch that exception and take the correct course of action.\n\n    try:\n        y = mySqrt(x)\n    except Exception:\n        y = 1j * mySqrt(abs(x))\nreturn y\n\nprint(mySqrtComplex(16))\nprint(mySqrtComplex(-16))\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\n\n\n\n2.3.4 Handling Other Exceptions\nYou might find you can get away without raising any exceptions in your code. However, scipy and numpy will raise exceptions, and knowing how to handle them can be useful. For example, a number of linear algebra routines in scipy.linalg cannot proceed if given a singular matrix. In this case, they will raise a numpy.linalg.LinAlgError exception : https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.LinAlgError.html#numpy.linalg.LinAlg As well as the built-in Exception, Python allows us to define dedicated exception types, like this one. This allows exception handling code to distinguish different classes of error condition, which is useful when deciding what to do. An example of how to catch this kind of exception is below.\n\nimport scipy.linalg\nm = np.zeros((2,2))\nprint(m)\ntry:\n    scipy.linalg.inv(m)\nexcept scipy.linalg.LinAlgError as err:\n    print(\"Caught an exception :\", err)\n\n[[0. 0.]\n [0. 0.]]\nCaught an exception : singular matrix",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Advanced Python Programming</span>"
    ]
  },
  {
    "objectID": "linear-algebra.html",
    "href": "linear-algebra.html",
    "title": "3  Linear Algebra",
    "section": "",
    "text": "3.1 Basic Matrix Operations\nPython/numpy/scipy provide a range of options for achieving basic matrix operations. You will need to take a little care to ensure that your code implements the operations you intend it to. This is largely due to the fact that some operators/functions will change their behaviour depending on the input you provide. This section includes some recommendations for simple linear algebra, which should ensure your code behaves as desired.\nMatrices can be implemented as a 2D np.ndarray. Basic matrix arithmetic can then be performed using standard operators +,- and @. You can also use np.matmul() for matrix multiplication. Numpy will also perform matrix multiplication with np.dot(), but this is not recommended if you can use @ or np.matmul().\nimport numpy as np\n\nA = np.array([[1, 2, 3,],[4, 5, 6],[1, 0, 0]])\nB = np.array([[1, 0, 0,],[0, 1, 0],[0, 0, 1]])\n\n# addition\nprint(A + B)\n\n# subtraction\nprint(A - B)\n\n# scalar multiplication\nprint(3*B)\n\n# matrix multiplication\nprint(np.matmul(A, B))\nprint(A @ B)\n\n[[2 2 3]\n [4 6 6]\n [1 0 1]]\n[[ 0  2  3]\n [ 4  4  6]\n [ 1  0 -1]]\n[[3 0 0]\n [0 3 0]\n [0 0 3]]\n[[1 2 3]\n [4 5 6]\n [1 0 0]]\n[[1 2 3]\n [4 5 6]\n [1 0 0]]\nVectors can be implemented as 1D arrays, or as 2D arrays. A 1D array will be interpreted as row or column vector depending on the context in which it is used. Use of 2D arrays allows you to specify row or column form. This can be useful, since np.matmul() or @ will throw an exception if you accidentally try to perform an illegal matrix operation.\nv  = np.array([1,2,3])\nvr = np.array([[1,2,3]])\nvc = np.array([[1],[2],[3]])\n\n# two options for matrix * vector\nprint(A@v)\nprint(A@vc)\n\n# two options for vector * matrix\nprint(v@A)\nprint(vr@A)\n\n# this is not a valid matrix multiplication !\nprint(A@vr)\n\n[14 32  1]\n[[14]\n [32]\n [ 1]]\n[12 12 15]\n[[12 12 15]]\n\n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 1 is different from 3)\nNumpy will also provide the usual forms of vector product via np.vdot(), np.cross(), np.inner() and np.outer(). Again, np.dot() will provide a vector dot product, but is not recommended if you can use vdot().\nOther useful matrix operations are provided by numpy, such as : - np.transpose() (also available via ndarray.T) - np.norm() - np.trace()\nFor further information, look at the reference pages : https://numpy.org/doc/stable/reference/routines.array-manipulation.html https://numpy.org/doc/stable/reference/routines.linalg.html\nFinally, scipy.linalg provides some additional basic operations such as the determinant and the inverse.\nimport numpy as np\nimport scipy.linalg as linalg\n\nA = np.array([[1, 2, 3,],[4, 5, 6],[1, 0, 0]])\ndetA = linalg.det(A)\nprint(detA)\n\ninvA = linalg.inv(A)\nprint(invA)\n\n-3.000000000000001\n[[-0.          0.          1.        ]\n [-2.          1.         -2.        ]\n [ 1.66666667 -0.66666667  1.        ]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "linear-algebra.html#simultaneous-equations",
    "href": "linear-algebra.html#simultaneous-equations",
    "title": "3  Linear Algebra",
    "section": "3.2 Simultaneous Equations",
    "text": "3.2 Simultaneous Equations\nMany problems in physics require solving simultaneous equations. When these become large and complex, numerical routines are required.\nA set of simultaneous equations can always be written in matrix form, for example, two equations in two unknowns (\\(x_1\\) and \\(x_2\\))\n\\[\n\\begin{aligned}\nax_1 + bx_2 &= y_1 \\\\\ncx_1 + dx_2 &= y_2\n\\end{aligned}\n\\tag{3.1}\\]\ncan be rewritten as\n\\[\n\\left(\\begin{array}{cc} a & b \\\\ c & d\\end{array}\\right)\n\\left(\\begin{array}{c} x_1 \\\\ x_2 \\end{array}\\right) =\n\\left(\\begin{array}{c} y_1 \\\\ y_2 \\end{array}\\right)\n\\tag{3.2}\\]\nAn arbitrary set of equations is\n\\[Ax = y \\tag{3.3}\\]\nwhere A is the matrix of coefficients, x is the vector of unknown variables \\(x_1\\), \\(x_2\\), … and y is the known vector of constants.\n\n3.2.1 Inverse Matrix\nOne way to solve the above equation is to multiply both sides by the inverse of A:\n\\[A^{-1} A x = A^{-1} y \\tag{3.4}\\]\ngiving :\n\\[x = A^{-1} y \\tag{3.5}\\]\nThis is demonstrated in the example below for a simple test case :\n\\[\n\\left(\\begin{array}{ccc} 1 & 2 & 2 \\\\\n                        3 & 1 & 6 \\\\\n                        0 & 2 & 2\\end{array}\\right)\n\\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right) =\n\\left(\\begin{array}{c} 2 \\\\ 7 \\\\ 1\\end{array}\\right)\n\\tag{3.6}\\]\n\nimport numpy as np\nimport scipy.linalg as linalg\n\ndef solve_inv(a,y):\n    x = linalg.inv(a) @ y\n    return x\n\na = np.array([[1, 2, 2,],[3, 1, 6],[0, 2, 2]])\ny = np.array([[2], [7], [1]])\nprint(a)\nprint(y)\n\nx = solve_inv(a,y)\nprint(x)\n\n[[1 2 2]\n [3 1 6]\n [0 2 2]]\n[[2]\n [7]\n [1]]\n[[ 1. ]\n [-0.2]\n [ 0.7]]\n\n\nIs this the solution? We can easily check by inserting the solution into the original equation.\n\nprint(a @ x)\n\n[[2.]\n [7.]\n [1.]]\n\n\nWhich is indeed equal to our y above. This kind of test is known as a ‘closure test’ and will be used frequently throughout this unit to verify our code.\nBefore using this method for solving simultaneous equations, though, we should understand how scipy.linalg.inv finds the matrix inverse. Unfortunately, this is tricky to understand from the reference page, (scipy.linalg.inv). But, if you examine the source code for this function you’ll see that it uses a function called DGETRI. This is defined in the LAPACK library, and its reference page is here. As you can see this routine uses LU decomposition to find the inverse! It doesn’t make sense, therefore, to find a matrix inverse simply to solve a simultaneous equation, and using LU decomposition directly will involve fewer operations. However, there are exceptions when dealing with many simultaneous equations. For example, suppose you have a sequence of problems which all feature the same matrix \\(A\\), but have different RHS \\(y\\). In this case it would be efficient to invert \\(A\\) once, then multiply by \\(y\\) to solve each problem, since multiplication involves fewer operations than LU decomposition.\nAs an aside, LAPACK is a linear algebra library written in FORTRAN - which remains one of the most efficient languages for writing numerical methods - and most routines in scipy.linalg basically provide a Python interface to this library.\n\n\n3.2.2 Gaussian Elimination\nSome sets of simultaneous equations are easy to solve. For example :\n\\[\n\\begin{aligned}\na x_1 &= y_1 \\\\\nb x_2 &= y_2 \\\\\nc x_3 &= y_3\n\\end{aligned}\n\\tag{3.7}\\] This can be written in what is known as row echelon form :\n\\[\n\\left(\\begin{array}{ccc} a & 0 & 0 \\\\\n                        0 & b & 0 \\\\\n                        0 & 0 & c\\end{array}\\right)\n\\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right) =\n\\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3\\end{array}\\right)\n\\tag{3.8}\\]\nAnd then reduced row echelon form :\n\\[\n\\left(\\begin{array}{ccc} 1 & 0 & 0 \\\\\n                        0 & 1 & 0 \\\\\n                        0 & 0 & 1\\end{array}\\right)\n\\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right) =\n\\left(\\begin{array}{c} y_1/a \\\\ y_2/b \\\\ y_3/c\\end{array}\\right)\n\\tag{3.9}\\]\nGauss-Jordan elimination is a process which reduces any linear equation set to this form. It can be shown that the reduced row echelon form is unique, and therefore independent of the order of operations which are used to find it. The technique is illustrated using the example problem from earlier. It’s convenient to use the ‘augmented’ matrix, which includes the right-hand side. \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 2 & 2 & 2 \\\\\n    3 & 1 & 6 & 7 \\\\\n    0 & 2 & 2 & 1\n\\end{array}\\right)\n\\tag{3.10}\\]\nThen we apply simple operations until we obtain the equation in row echelon form. These operations include:\n\nMultiply a row by a constant\nSwap two rows\nSum two rows in a linear combination\n\n(Hopefully, these sound reasonably familiar - we are just formalising techniques you will have used before)\nReplace \\(R_1\\) (row 1) with \\(R_1 - R_3\\) : \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 0 & 0 & 1 \\\\\n    3 & 1 & 6 & 7 \\\\\n    0 & 2 & 2 & 1\n\\end{array}\\right)\n\\tag{3.11}\\]\nReplace \\(R_2\\) with \\(R_2 - 3R_1\\) : \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 0 & 0 & 1 \\\\\n    0 & 1 & 6 & 4 \\\\\n    0 & 2 & 2 & 1\n\\end{array}\\right)\n\\tag{3.12}\\]\nReplace \\(R_2\\) with \\(R_2 - \\frac{2}{5}R_2\\) : \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 0 & 0 & 1 \\\\\n    0 & -5 & 0 & 1 \\\\\n    0 & 0 & 2 & \\frac{7}{5}\n\\end{array}\\right)\n\\tag{3.13}\\]\nAnd then finally for reduced row echelon form, replace \\(R_2\\) with \\(\\frac{-1}{5}R_2\\) and \\(R_3\\) with \\(\\frac{1}{2}R_3\\) \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 0 & 0 & 1 \\\\\n    0 & 1 & 0 & -\\frac{1}{5} \\\\\n    0 & 0 & 1 & \\frac{7}{10}\n\\end{array}\\right)\n\\tag{3.14}\\]\nSo the solution is : \\[\n\\begin{aligned}\nx_1 &= 1 \\\\\nx_2 &= -\\frac{1}{5} \\\\\nx_3 &= \\frac{7}{10}\n\\end{aligned}\n\\tag{3.15}\\]\n\n\n3.2.3 LU Decomposition\nMatrix decomposition techniques involve factorising a general matrix into a product of several matrices. LU decomposition involves writing the general matrix, \\(A\\), as the product of two triangular matrices, \\(L\\) and \\(U\\).\n\\[\nA=\n\\left(\\begin{array}{ccc} a_{11} & a_{12} & a_{13} \\\\\n                         a_{21} & a_{22} & a_{23} \\\\\n                         a_{31} & a_{32} & a_{33}\n\\end{array}\\right)\n\\tag{3.16}\\]\n\\[\nA=LU=\n\\left(\\begin{array}{ccc} 1      & 0      & 0 \\\\\n                         l_{21} & 1      & 0 \\\\\n                         l_{31} & l_{32} & 1\n\\end{array}\\right)\n\\left(\\begin{array}{ccc} u_{11} & u_{12} & u_{13} \\\\\n                         0      & u_{22} & u_{23} \\\\\n                         0      & 0      & u_{33}\n\\end{array}\\right)\n\\tag{3.17}\\]\nWe can use LU decomposition to solve matrix equations since it allows us to write the equation \\[Ax = y\\] as \\(L(Ux)=y\\). This can then be written as two equations \\(Lc=y\\) and \\(Ux=c\\), which are trivially solved, first for \\(c\\), and then for \\(x\\).\nThe matrices \\(L\\) and \\(U\\) can be found using the operations described above for Gaussian elimination. There are several algorithmic formulations that define the sequence of operations. Scipy provides an LU decomposition routine: scipy.linalg.lu(). Note that this performs a variation on the LU decomposition described above, since it also computes a permutation matrix \\(P\\), such that\n\\[PA = LU \\tag{3.18}\\]\nScipy also provides a simple function to obtain the solutions to a matrix equation. scipy.linalg.lu_solve() expects the \\(L\\), \\(U\\) and \\(P\\) matrices as arguments, as shown in the example below.\n\nscipy.linalg.lu\nscipy.linalg.lu_solve\n\n\ndef solve_lu(a,y):\n    lu, piv = linalg.lu_factor(a)\n    x = linalg.lu_solve((lu, piv), y)\n    return x\n\nprint(solve_lu(a,y))\n\n[[ 1. ]\n [-0.2]\n [ 0.7]]\n\n\nNote that the general purpose solvers provided by both numpy and scipy both utilise LU decomposition :\n\nnumpy.linalg.solve\nscipy.linalg.solve\n\n\n\n3.2.4 SVD Decomposition\nLU decomposition will find an exact solution to the matrix equation in a wide variety of cases. However, a solution may not exist, or there may be infinite solutions. In such cases, the Singular Value Decomposition may be of use.\nFor an \\(m \\times n\\) matrix \\(A\\), the singular values, \\(\\sigma\\) are given by the solutions to\n\\[\n\\begin{aligned}\nAv &= \\sigma u \\\\\nA^Tu &=\\sigma v\n\\end{aligned}\n\\tag{3.19}\\]\nwhere \\(u\\) and \\(v\\) are two non-zero vectors. These equations are closely related to the eigenvalue equation. Indeed, the singular values are also the square roots of the eigenvalues of \\(A^TA\\).\nThe singular value decomposition of \\(A\\) is\n\\[A = U\\Sigma V^T \\tag{3.20}\\]\nwhere \\(U\\) and \\(V\\) are orthonormal matrices, and \\(\\Sigma\\) is a matrix with the singular values on its leading diagonal, and zero elsewhere.\nThe SVD decomposition allows use to compute the pseudo-inverse of \\(A\\), which is given by :\n\\[A^\\dagger = V \\Sigma^\\dagger U^T \\tag{3.21}\\]\nwhere \\(\\Sigma^\\dagger\\) is the pseudo-inverse of \\(\\Sigma\\) and is obtained by transposing \\(\\Sigma\\) and replacing each non-zero element with it’s reciprocal.\nThe pseudoinverse (also known as the Moore-Penrose inverse) can always be computed, even when \\(A\\) is singular, ie. when when \\(\\frac{1}{|A|}=0\\) and the inverse cannot be found.\nIn the context of solving a matrix equation \\(Ax=y\\), the product of pseudoinverse and the RHS, (i.e. \\(\\bar{X}=A^\\dagger y\\)) has various properties. When A is non-singular, \\(\\bar{x}\\) gives the solution to \\(Ax=y\\). When \\(A\\) is singular, \\(\\bar{x}\\) is a least squares approximation to the nearest solution. When \\(Ax=y\\) has a space of solutions (equivalent to a set of simultaneous equations with degeneracy), then \\(\\bar{x}\\) is a vector which describes this space.\nSVD decomposition is available in Scipy using scipy.linalg.svd(). For further information, see scipy.linalg.svd. Note that, unlike LU decomposition, no solve() function is supplied, and instead we must write some code to calculate \\(\\bar{x}\\).\n\ndef solve_svd(a,y):\n    u, s, v = linalg.svd(a)\n    x = v.T @ np.diag(1/s) @ u.T @ y\n    return x\n\nprint(solve_svd(a,y))\n\n[[ 1. ]\n [-0.2]\n [ 0.7]]\n\n\nAlternatively, a function to compute the pseudoinverse directly is provided, scipy.linalg.pinv.\n\nprint( linalg.pinv(a) @ y)\n\n[[ 1. ]\n [-0.2]\n [ 0.7]]\n\n\n\n\n3.2.5 Physics Example\nHere we illustrate the use of simultaneous equation solvers in a familiar context - the use of Kirchoff’s laws and Ohm’s law to understand resistor networks. Typically, analysis of a resistor network will involve solving simultaneous equations, to calculate voltage and current at the desired points in the network. Consider the electronic circuit shown in the diagram.\n\n\n\nFigure 1 - Example resistor network.\n\n\nWhere : \\(V_1 = 12V\\), \\(V_2 = 12V\\), \\(R_1 = 3 \\Omega\\), \\(R_2 = 3 \\Omega\\), \\(R_3 = 10 \\Omega\\), \\(R_4 = 2 \\Omega\\), \\(R_5 = 2 \\Omega\\).\nBy identifying the three current loops indicated, we can use Kirchoff’s loop rule and Ohm’s law to write : \\[\n\\begin{aligned}\nV_1 &= I_1 R_1 + (I_1 - I_2) R_2 \\\\\n0   &= (I_2 - I_1) R_2 + I_2 R_3 + (I_2 - I_3) R_4 \\\\\n-V_2 &= (I_3 - I_2) R_4 + I_3 R_5\n\\end{aligned}\n\\tag{3.22}\\]\nSo we have a set of simultaneous equations, which we can write as a matrix equation :\n\\[\n\\begin{pmatrix}\nR_1+R_2 & -R_2 & 0 \\\\\n-R_2 & R_2+R_3+R_4 & -R_4 \\\\\n0  & -R_4 & R_4+R_5 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nI_1 \\\\\nI_2 \\\\\nI_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nV_1 \\\\\n0 \\\\\n-V_2\n\\end{pmatrix}\n\\tag{3.23}\\]\nSolving this matrix equation will provide the current at all points in the circuit. This method is known as “mesh analysis” of circuits.\nWe can write a simple function that, given the voltage and resistor values, will return the currents :\n\ndef meshAnalysis(v1, v2, r1, r2, r3, r4, r5):\n    m = np.array ([[r1+r2, -r2, 0],[-r2,r2+r3+r4,-r4],[0,-r4,r4+r5]])\n    v = np.array([[v1],[0],[-v2]])\n    i = linalg.solve(m,v)\n    return i\n\nWhich, for the values given, will return the three currents :\n\ni = meshAnalysis(12, 12, 3, 3, 10, 2, 2)\n\nprint(i)\n\n[[ 2.]\n [ 0.]\n [-3.]]\n\n\nOr we could calculate, for example, how \\(I_2\\) will vary as a function of \\(R_4\\), with all other values fixed :\n\nr4s = np.linspace(0.1, 5.0, 100)\ni2s = np.empty(len(r4s))\n\nfor j,r4 in enumerate(r4s):\n    i     = meshAnalysis(12, 12, 3, 3, 10, r4, 2)\n    i2s[j] = i[1][0]\n\nimport matplotlib.pyplot as plt\n\nplt.plot(r4s, i2s)\nplt.ylabel('$I_2$ (A)')\nplt.xlabel('$R_4$ (ohm)')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "linear-algebra.html#eigenproblems",
    "href": "linear-algebra.html#eigenproblems",
    "title": "3  Linear Algebra",
    "section": "3.3 Eigenproblems",
    "text": "3.3 Eigenproblems\nA square NxN matrix \\(A\\), has eigenvector \\(u\\) and eigenvalue \\(\\lambda\\) that satisfy :\n\\[(A - \\lambda I)u = 0 \\tag{3.24}\\]\nSolving eigenproblems is closely related to finding the roots of polynomials. You may be familiar with one method for finding eigenvalues, which is to find the roots of the N-th degree polynomial found by expanding :\n\\[p(t) = \\det{|A - t I|} = 0 \\tag{3.25}\\]\nMany eigenproblem solving routines are provided by SciPy. In particular, scipy.linalg.eig(A) will return a tuple containing the eigenvalues and eigenvectors of A. If only the eigenvalues are required, scipy.linalg.eigenvals(A) can be used.\nCare should be taken when using scipy.linalg.eig, since it will find “left” and “right” eigenvectors, as specified, which are the solutions to \\(v A = \\lambda v\\) and \\(A v = \\lambda v\\) respectively.\nFor further details see scipy.linalg.eig\nAdditional routines include :\n\nscipy.linalg.eigh (for Hermitian matrices)\nscipy.linalg.eig_banded (for banded matrices)\nscipy.sparse.linalg.eigs (for sparse, square symmetric matrices)\n\n\n3.3.1 Simple example\nWe can test these sovlers using the matrix :\n\\[A =\n\\pmatrix{\n-2 & -4 & 2 \\\\\n-2 &  1 & 2 \\\\\n4  &  2 & 5}\n\\tag{3.26}\\]\nfor which the eigenvalues are \\(\\lambda^{(0)}=6\\), \\(\\lambda^{(1)}=-5\\), \\(\\lambda^{(2)}=3\\).\nNote that the algorithms discussed here will all find unit eigenvectors. The eigenvector corresponding to \\(\\lambda^{(0)}\\) is then :\n\\[\\hat{u}^{(0)}=\\pmatrix{\\frac{1}{\\sqrt{293}} \\\\\n\\frac{6}{\\sqrt{293}} \\\\\n\\frac{16}{\\sqrt{293}}\n}\n=\n\\pmatrix{0.058 \\\\\n0.351 \\\\\n0.935} \\tag{3.27}\\]\nwith numerical values given to 3 decimal places on the RHS.\n\nimport numpy as np\nimport scipy.linalg as linalg\n\nm = np.array([[-2,-4,2],[-2,1,2],[4,2,5]])\n\n# set seed for repeatability\nnp.random.seed(2)\n\n# run the algorithm\nmus, vs = linalg.eig(m)\n\n# print results\nnp.set_printoptions(precision=3)\nfor i in range(3):\n    print(\"Eigenvalue/vector : {:.1f} {}\".format(mus[i], vs.T[i]))\n\nEigenvalue/vector : -5.0+0.0j [ 0.816  0.408 -0.408]\nEigenvalue/vector : 3.0+0.0j [ 0.535 -0.802 -0.267]\nEigenvalue/vector : 6.0+0.0j [0.058 0.351 0.935]\n\n\nWhich includes the solution expected.\nNote that we have transposed the array of eigenvectors returned by linalg.eig(). This is a feature of the function, as described in the reference manual : “The normalized left eigenvector corresponding to the eigenvalue w[i] is the column vl[:,i]”.\n\n\n3.3.2 Physics Example\nIn this section, we illustrate the use of eigenvalue solvers in finding stable solutions of the coupled system of oscillators shown below.\n\n\n\nFigure 2 - Coupled oscillators, masses on springs.\n\n\nIf the displacement of the \\(i\\)th mass from its equilibrium position is denoted as \\(x_i\\), the force on the mass is given by the tension in the two springs as :\n\\[F_i = −k(x_i − x_{i−1}) + k(x_{i+1} − x_i) = −k(2x_i − x_{i−1} − x_{i+1}) \\tag{3.28}\\]\nWe can assume that there are normal mode solutions, i.e. solutions of the form \\(x_i = z_i e^{i\\omega t}\\) in which all masses oscillate with the same frequency \\(\\omega\\) but with unknown phasors \\(z_i\\). Then the above equation becomes :\n\\[F_i = m\\ddot{x}_i = −m\\omega^2x_i = −k(2x_i − x_{i−1} − x_{i+1}) \\tag{3.29}\\]\nThis is one row of a matrix equation describing the entire system :\n\\[m\\omega^2x_i \\left(\\begin{array}{c} \\vdots \\\\ \\\\ x_i \\\\ \\\\ \\vdots \\end{array}\\right) =\n\\left(\\begin{array}{ccccccc} & & & \\vdots & & & \\\\ \\cdots & 0 & -1 & 2 & -1 & 0 & \\cdots \\\\ & & & \\vdots & & & \\end{array}\\right)\n\\left(\\begin{array}{c} \\vdots \\\\ x_{i-1} \\\\ x_i \\\\ x_{i+1} \\\\ \\vdots \\end{array}\\right)\n\\tag{3.30}\\]\nThis example is a typical eigenvalue problem, in that many of the matrix elements are zero, which can greatly simplify the computational challenge and make even large systems solvable. The matrix is symmetric, which means it is suitable for solving with our eigenproblem solving function above, or one of the solvers from scipy.linalg.\n\nm = np.array([[2, -1,  0,  0,  0,  0,  0],\n              [-1, 2, -1,  0,  0,  0,  0],\n              [0, -1,  2, -1,  0,  0,  0],\n              [0,  0, -1,  2, -1,  0,  0],\n              [0,  0,  0, -1,  2, -1,  0],\n              [0,  0,  0,  0, -1,  2, -1],\n              [0,  0,  0,  0,  0, -1,  2]])\n\nmus, vs = linalg.eig(m)\n\nThe eigenvalue associated with each mode gives the frequency, while the (complex) eigenvector provides the magnitude and phase of oscillation for each mass. We can plot the displacement of each mass as a function of time for each mode.\n\nimport numpy as np\nimport scipy.linalg as linalg\nimport matplotlib.pyplot as plt\n\n# a function to calculate the (real) displacement from complex phase\ndef disp(zi, omega, t):\n    return np.real(zi * np.exp(1j * omega * t))\n\n# set up some pretty colours for plotting\ncm  = plt.cm.viridis\ncol = [cm(int(x*cm.N/7)) for x in range(7)]\n\n# time period\nts = np.arange(0,40, 0.001)\n\n# loop over eigenmodes\nfor i in range(7):\n    \n    print(\"Mode       : \",i)\n    print(\"Eigenvalue : \", mus[i])\n\n    fig=plt.figure(figsize=(16, 4))\n\n    xs = []\n    \n    # loop over masses\n    for j in range(7):\n        \n        # get the displacement, and add an offset to separate out each line\n        offset = (2*j)-6\n        \n        # create displacement values from function using eigenvectors and eigenvalues\n        xs     = disp(vs.T[i][j], mus[i], ts) + offset\n        \n        # plot displacement\n        plt.plot(ts, xs, color=col[j])\n        \n        # plot central position to guide the eye\n        plt.plot([0, 40], [offset, offset], color=col[j], linestyle='dotted') \n\n    plt.xlabel(\"t\")\n    plt.show()\n\nMode       :  0\nEigenvalue :  (3.8477590650225726+0j)\nMode       :  1\nEigenvalue :  (3.4142135623730923+0j)\nMode       :  2\nEigenvalue :  (2.765366864730178+0j)\nMode       :  3\nEigenvalue :  (1.9999999999999984+0j)\nMode       :  4\nEigenvalue :  (0.1522409349774269+0j)\nMode       :  5\nEigenvalue :  (0.585786437626905+0j)\nMode       :  6\nEigenvalue :  (1.2346331352698203+0j)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "monte-carlo.html",
    "href": "monte-carlo.html",
    "title": "4  Monte Carlo Methods",
    "section": "",
    "text": "4.1 Pseudo Random Number Sampling\nWe will frequently want to generate random numbers according to a particular probability distribution, or in the jargon ‘sampling’ from the given distribution. There are two basic methods for achieving this; an analytical method, and the accept/reject method.\nIn the below, we assume we are provided with a random number generator that returns pseudo-random numbers with a uniform probability distribution in the interval \\([0,1)\\). In Python, numpy.random provides the random() function, which does exactly this. A variety of other probability distributions are provided by numpy.random, and in general these should be used when possible. However, the techniques described here allow any desired PDF to be sampled.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "monte-carlo.html#pseudo-random-number-sampling",
    "href": "monte-carlo.html#pseudo-random-number-sampling",
    "title": "4  Monte Carlo Methods",
    "section": "",
    "text": "4.1.1 Uniform Probability Distributions\nWe will often need uniform probability distributions over an interval other than \\([0,1)\\). It is straightforward to map this interval to the desired one, as shown in the example below.\n\nimport numpy.random as random\nimport matplotlib.pyplot as plt\n\n# produce random numbers in the range 150-250\na = 100*random.random(int(1e5))+150\nplt.hist(a)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.1.2 Analytical Method\nThe above transformation is a special case of the method described in this section. We can consider a random number generator that produces values x over the interval \\((x_1, x_2)\\) with probability \\(P(x)\\), which we wish to convert to values y on the interval \\((y_1, y_2)\\), with probability \\(P'(y)\\). To construct a transformation from a generated value \\(x_{in}\\) to an output value with the required distribution, \\(y_{out}\\), we require that the cumulative distributions are equal :\n\\[\\int_{x_0}^{x_{in}} P(x) dx = \\int_{y_0}^{y_{out}} P'(y) dy \\tag{4.1}\\]\nNote that for \\(x_{in} = x_2\\), \\(y_{out} = y_2\\) both integrals must equal 1.\nIf the LHS of the above equation is uniform on the interval \\([0,1)\\), then we have :\n\\[x_{in} = \\int_{y_0}^{y_{out}} P'(y) dy \\tag{4.2}\\]\nIf we then define the function \\(Q(y_{out})\\) such that :\n\\[Q(y_{out}) = x_{in} = \\int_{y_0}^{y_{out}} P'(y) dy \\tag{4.3}\\]\nThen the transformation we must apply to \\(x_{in}\\), in order to obtain \\(y_{out}\\), is simply the inverse function, ie :\n\\[y_{out} = Q^{-1}(x_{in}) \\tag{4.4}\\]\n\n\n\n4.1.3 Analytical Method Example\nIn this example, we will write a function to produce values \\(y\\) in the interval \\([0, \\pi)\\) with probability distribution proportional to \\(\\sin(y)\\).\nHere, the integral above becomes :\n\\[\n\\begin{aligned}\nQ(y_{out}) &= \\frac{1}{2}\\int_{0}^{y_{out}} \\sin(y) dy \\\\\n           &= -\\frac{1}{2}\\cos(y_{out}) + C\n\\end{aligned}\n\\tag{4.5}\\]\nNote the factor \\(\\frac{1}{2}\\) is required to ensure the integral from \\(0\\) to \\(\\pi\\) is equal to 1. We can determine the constant of integration by requiring that \\(Q(0) = 0\\), for \\(x_{in}=0\\) and \\(Q(\\pi)=1\\) for \\(x_{in}=1\\).\nWe then find that\n\\[Q(y_{out}) = -\\frac{1}{2}\\cos(y_{out}) + \\frac{1}{2} \\tag{4.6}\\]\nAnd our inverse transformation is :\n\\[y_{out} = Q^{-1}(x_{in}) = \\cos^{-1}(1-2x_{in})  \\tag{4.7}\\]\n\ndef randSinAna():\n    \"\"\"Generate a random theta between 0 and pi, with PDF sin(theta) using analytical method\"\"\"\n    x = random.random()\n    return np.arccos(1-2*x)\n\n\nimport numpy as np\n\n# generate 50,000 points using a list comprehension\nn1s = [randSinAna() for _ in range(50000)]\n\n# plot a histogram\nhist1, bins1, patches1 = plt.hist(n1s, bins=50, density=True, label=\"Analytic Method\")\n\n# plot a sin(theta) function for comparison\nbin_centres = (bins1[1:] + bins1[:-1])/2\nplt.plot(bin_centres, np.sin(bin_centres)/2, label=r'$sin(\\theta)$')\n\nplt.xlabel(r'$\\theta$')\nplt.ylabel(r'$P(\\theta)$')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nIt might be worth highlighing the method used here (and in examples below) to generate a large number of points. This is a list comprehension. The list is generated by calling the first argument in the square brackets (here randSinAna()) for every iteration of the for loop. The for loop uses the underscore instead of a loop variable, since no variable is required. But in other cases, one could use a standard for loop to generate values in a list, eg :\n\n[i for i in range(5)]\n\n[0, 1, 2, 3, 4]\n\n\n\n\n\n4.1.4 Accept/Reject Method\nFor some PDFs, the integral required by the previous method cannot be determined analytically. In such cases, the accept/reject method provides a simple alternative. This method involves three steps : 1. a random number, \\(y\\), is generated in the desired interval \\((y_1, y_2)\\) 2. a second random number, \\(p\\), is generated between 0 and the maximum value of \\(P'(y)\\) 3. if \\(p &lt; P'(y)\\) then \\(y\\) is returned, otherwise it is rejected and the process is repeated\nThis method is clearly less efficient than the analytical method, since two random numbers are generated for each number returned, and some fraction of these are rejected. However, it allows us to generate any arbitrary probability distribution.\n\n\n\n4.1.5 Accept/Reject Example\nHere we demonstrate the accept/reject method for the same example as above, to produce values \\(y\\) in the interval \\([0, \\pi)\\), with probability distribution proportional to \\(\\sin(y)\\).\n\ndef randSinAR():\n    \"\"\"Generate a random theta between 0 and pi, with PDF sin(theta) using accept/reject method\"\"\"\n    while True:\n        x = np.pi * np.random.random()\n        y = np.random.random()\n        if y &lt; np.sin(x):\n            return x\n        else:\n            continue\n\n\n# generate 50,000 points using a list comprehension\nn2s = [randSinAR() for _ in range(50000)]\n\n# plot a histogram from the analytic method\nhist1, bins1, patches1 = plt.hist(n1s, bins=50, density=True, label=\"Analytic Method\")\n\n# plot another histogram from the accept/reject method\nhist2, bins2, patches2 = plt.hist(n2s, bins=bins1, density=True, label=\"Accept/reject\")\n\n# and the sin(theta) function for comparison\nplt.plot(bin_centres, np.sin(bin_centres)/2, label=r'$sin(\\theta)$')\n\nplt.xlabel(r'$\\theta$')\nplt.ylabel(r'$P(\\theta)$')\nplt.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "monte-carlo.html#multivariate-sampling",
    "href": "monte-carlo.html#multivariate-sampling",
    "title": "4  Monte Carlo Methods",
    "section": "4.2 Multivariate Sampling",
    "text": "4.2 Multivariate Sampling\nBoth analytic and accept/reject methods can be used to generate distributions of more than one variable. This often requires a bit of thought - the two methods may be more or less suitable to particular problems.\nHere we illustrate the two approaches to generating uniformly distributed random numbers on the unit disc.\n\n4.2.1 Analytic Example\nAn analytic method for the unit disc problem needs to ensure that the density of points is constant over the disc, ie that \\(P(x,y) \\propto dA\\) for area element \\(dA\\). In polar coordinates, we can write this as :\n\\[P(x,y) \\propto dA = r dr d\\phi\\]\nSince we will start by generating values with uniform distributions (let’s say \\(u\\) and \\(v\\)), we want to obtain transformations \\((u,v) \\rightarrow (r, \\phi)\\) such that :\n\\[dA = r dr d\\phi= du dv\\]\nClearly these substitutions are sufficient :\n\\[du = r dr\\] \\[dv = d\\phi\\]\nClearly we can just generate \\(\\phi\\) with a uniform distribution. The function to produce \\(r\\) from uniformly distributed \\(u\\) is obtained by integration :\n\\[u = \\frac{1}{2}r^2\\]\nand\n\\[r = \\sqrt{2u}\\]\nHowever, this will produce a disc with incorrect area. The required area is \\(\\pi\\), so we can obtain the constant of integration by requiring \\(\\int dA = \\pi\\), which gives :\n\\[r = \\sqrt{u}\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef unitDiscAna():\n    phi = 2 * np.pi * np.random.random()\n    r = np.sqrt(np.random.random())\n    \n    # convert to cartesian coordinates\n    x = r * np.cos(phi)\n    y = r * np.sin(phi)\n    \n    return np.array([x, y])\n\nps = np.array([unitDiscAna() for _ in range(1000)])\n\nplt.axis('equal')\nplt.scatter(ps[:,0], ps[:,1], marker='.', c='r')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.2.2 Accept/Reject Example\nSuppose we want to randomly generate points \\((x,y)\\) within a unit disc. A simple approach is to generate uniform distributions of \\(x\\) and \\(y\\) separately, and then use an accept/reject method to remove any points not in the disc (ie. where \\(\\sqrt{x^2 + y^2} \\gt 1\\)). This is illustrated in the example below.\n\ndef unitDiscAR():\n    x = 2 * np.random.random() - 1\n    y = 2 * np.random.random() - 1\n    while np.sqrt(x**2+ y**2) &gt; 1:\n        x = 2 * np.random.random() - 1\n        y = 2 * np.random.random() - 1\n    return np.array([x, y])\n\nps = np.array([unitDiscAR() for _ in range(1000)])\n\nplt.axis('equal')\nplt.scatter(ps[:,0], ps[:,1], marker='.', c='r')\nplt.show()\n\n\n\n\n\n\n\n\nNote that since the unitDisc() method returns a vector, we need to :\n\nconvert the list generated by the list comprehension (which calls unitDisc() many times) into a 2D array\nuse array slicing to obtain arrays of \\(x\\) and \\(y\\) values separately when plotting, ie. ps[:,0] gives a 1D array of \\(x\\) values.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "intro-ml.html",
    "href": "intro-ml.html",
    "title": "5  Introduction to Machine Learning",
    "section": "",
    "text": "5.1 What is Machine Learning ?\nMachine learning is the study of algorithms that enable computers to learn patterns or relationships from data and make predictions or decisions without being explicitly programmed.\nCore idea: Instead of coding physics laws explicitly, we let algorithms infer structure or relationships.\nTraditional physics → build models from first principles. Machine learning → learn models from data when physical models are incomplete, complex, or computationally expensive.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "intro-ml.html#examples-in-physics",
    "href": "intro-ml.html#examples-in-physics",
    "title": "5  Introduction to Machine Learning",
    "section": "5.2 Examples in Physics",
    "text": "5.2 Examples in Physics\n\n\n\n\n\n\n\nArea\nExample Use\n\n\n\n\nHigh-energy physics\nEvent classification in particle detectors\n\n\nCondensed matter\nDetecting phase transitions in spin models\n\n\nAstrophysics\nGalaxy morphology classification, transient detection\n\n\nQuantum physics\nNeural-network quantum states (Carleo & Troyer, 2017)\n\n\nPlasma physics\nML-based control of magnetic confinement\n\n\nComputational materials science\nSurrogate models for interatomic potentials",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "intro-ml.html#types-of-learning",
    "href": "intro-ml.html#types-of-learning",
    "title": "5  Introduction to Machine Learning",
    "section": "5.3 Types of learning",
    "text": "5.3 Types of learning\n\n\n\n\n\n\n\n\n\nCategory\nGoal\nExample Task\nExample Physics Application\n\n\n\n\nSupervised Learning\nLearn mapping (x y) from labelled data\nRegression, classification\nPredict energy levels from system parameters; classify detector events\n\n\nUnsupervised Learning\nDiscover structure in unlabelled data\nClustering, dimensionality reduction\nIdentify phases of matter, cluster spectral data\n\n\nReinforcement Learning\nLearn by interacting with environment\nSequential decision-making\nControl plasma confinement, optimize experiments",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "intro-ml.html#opportunities-challenges-ethics",
    "href": "intro-ml.html#opportunities-challenges-ethics",
    "title": "5  Introduction to Machine Learning",
    "section": "5.4 Opportunities, Challenges, Ethics",
    "text": "5.4 Opportunities, Challenges, Ethics\nOpportunities:\nHandle high-dimensional, noisy data.\nSpeed up simulations (surrogate modeling).\nEnable discovery in large datasets.\nChallenges:\nInterpretability — what does the ML model “understand”?\nOverfitting — learning noise, not physics.\nData bias and uncertainty quantification.\nEthical use — transparency, reproducibility, sustainability.\nHow do we ensure scientific rigor when using ML in physics?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html",
    "href": "supervised-ml.html",
    "title": "6  Supervised Learning",
    "section": "",
    "text": "6.1 Supervised Learning Framework\nTraining set, model, loss, optimisation, validation",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#regression",
    "href": "supervised-ml.html#regression",
    "title": "6  Supervised Learning",
    "section": "6.2 Regression",
    "text": "6.2 Regression\nLinear regression\nMinimisation of mean squared error\nConnection to maximum likelihood estimation under Gaussian noise\nPolynomial regression and basis expansion\nRegularisation (L2, L1)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#classification",
    "href": "supervised-ml.html#classification",
    "title": "6  Supervised Learning",
    "section": "6.3 Classification",
    "text": "6.3 Classification\nLogistic regression\nNegative log-likelihood loss (cross-entropy)\nConnection to MLE under Bernoulli model\nAlternatives : k-Nearest Neighbours (KNN). Support Vector Machines (SVMs)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#evaluating-model-performance",
    "href": "supervised-ml.html#evaluating-model-performance",
    "title": "6  Supervised Learning",
    "section": "6.4 Evaluating model performance",
    "text": "6.4 Evaluating model performance\nRegression metrics. MSE, RMSE, \\(R^2\\)\nClassification metrics. accuracy, precision, recall, F1 score, ROC curves.\nTrain/test split. Cross-validation.\nDemonstrate visually how overfitting and underfitting appear in training vs. validation error plots.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#bias-variance-overfitting",
    "href": "supervised-ml.html#bias-variance-overfitting",
    "title": "6  Supervised Learning",
    "section": "6.5 Bias, Variance, Overfitting",
    "text": "6.5 Bias, Variance, Overfitting\nDefine bias–variance trade-off.\nOverfitting in physics context: model fits noise rather than real phenomena.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "neural-networks.html",
    "href": "neural-networks.html",
    "title": "7  Neural Networks",
    "section": "",
    "text": "7.1 Anatomy of a Neural Network\nLayers: Input → Hidden layers → Output.\nEach neuron computes… Introduce weights, bias, act8ivation function",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#activation-function",
    "href": "neural-networks.html#activation-function",
    "title": "7  Neural Networks",
    "section": "7.2 Activation Function",
    "text": "7.2 Activation Function\nSigmoid, ReLU, tanh",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#forward-pass",
    "href": "neural-networks.html#forward-pass",
    "title": "7  Neural Networks",
    "section": "7.3 Forward Pass",
    "text": "7.3 Forward Pass\nCompute the output from input. Calculate loss (MSE, cross-entropy)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#backpropagation",
    "href": "neural-networks.html#backpropagation",
    "title": "7  Neural Networks",
    "section": "7.4 Backpropagation",
    "text": "7.4 Backpropagation\nUse chain rule to compute gradients of loss w.r.t. weights.\nIntuitive derivation, no need for full algebraic proof.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#gradient-descent",
    "href": "neural-networks.html#gradient-descent",
    "title": "7  Neural Networks",
    "section": "7.5 Gradient Descent",
    "text": "7.5 Gradient Descent\nRule for updating weights",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#training",
    "href": "neural-networks.html#training",
    "title": "7  Neural Networks",
    "section": "7.6 Training",
    "text": "7.6 Training\nBatch vs stochastic gradient descent Loss evolution: training loss vs. validation loss → early stopping to prevent overfitting.\nHyperparameters: learning rate, network depth, regularization, batch size.\nRegularization techniques:\nL2 weight decay\nDropout\nEarly stopping",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#deep-learning",
    "href": "neural-networks.html#deep-learning",
    "title": "7  Neural Networks",
    "section": "7.7 Deep Learning",
    "text": "7.7 Deep Learning\nMultiple hidden layers allow hierarchical feature extraction. Early layers learn simple features (edges, patterns). Deeper layers combine them into complex representations. Featurisation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "unsupervised-ml.html",
    "href": "unsupervised-ml.html",
    "title": "8  Unsupervised Learning",
    "section": "",
    "text": "8.1 Clustering\nK-means clustering Hierarchical clustering. DBSCAN.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised-ml.html#dimensionality-reduction",
    "href": "unsupervised-ml.html#dimensionality-reduction",
    "title": "8  Unsupervised Learning",
    "section": "8.2 Dimensionality Reduction",
    "text": "8.2 Dimensionality Reduction\nPCA t-SNE UMAP",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised-ml.html#evaluation-interpretation",
    "href": "unsupervised-ml.html#evaluation-interpretation",
    "title": "8  Unsupervised Learning",
    "section": "8.3 Evaluation & Interpretation",
    "text": "8.3 Evaluation & Interpretation\nNo ground truth: how do we know if unsupervised learning succeeded?\nUse internal validation (e.g., silhouette score for clustering).\nCompare discovered patterns to known physics features.\nInterpretability:\nPCA axes may correspond to physical modes or may not.\nClusters may or may not align with real physical classes.\nVisualization:\n2D projections, cluster coloring, dendrograms.\nDemonstrate how PCA + k-means can reveal structure in physical datasets.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  }
]