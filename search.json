[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Computational Physics & Machine Learning",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Pre-requisites\nWelcome to the course notes for Advanced Computational Physics and Machine Learning.\nThese notes will introduce the material covered in lectures and problems classes. They will be supplemented by lecture slides and example programming problems.\nThe notes will be released in four blocks for each half teaching block. The first block will cover numerical methods, as well as some advanced programming techniques :\nFuture blocks will cover further data science techniques, machine learning and high performance computing.\nThese notes assume you have studied computational physics up to 2nd year (Level 5), such as :",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#pre-requisites",
    "href": "intro.html#pre-requisites",
    "title": "1  Introduction",
    "section": "",
    "text": "PHYS20035 - Computational Physics and Data Science\nSCIF20002 - Programming and Data Analysis for Scientists",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "adv-python.html",
    "href": "adv-python.html",
    "title": "2  Advanced Python Programming",
    "section": "",
    "text": "2.1 Object Oriented Programming\nIn this chapter, we introduce some more advanced programming concepts. These include two important programming paradigms : object oriented programming and functional programming. These are both fairly advanced concepts, and are the subject of entire units in Computer Science degrees. Note that you are NOT expected to use these techniques in your work. Instead, they are introduced here by way of example, since you are likely to encounter and/or use them in your future programming journey.\nThis chapter will addresses exception handling - the main modern technique for managing errors - as well as the use of generative AI for programming.\nObject oriented programming is a programming paradigm that organises code around objects rather than functions and logic. Here we introduce some of the fundamental concepts (which you may have encountered before) including classes and objects. The four core principles of OO programming are then introduced, followed by some practical tips.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Advanced Python Programming</span>"
    ]
  },
  {
    "objectID": "adv-python.html#object-oriented-programming",
    "href": "adv-python.html#object-oriented-programming",
    "title": "2  Advanced Python Programming",
    "section": "",
    "text": "2.1.1 Classes & Objects\nUnsurprisingly, the fundamental building block of OO programming is the ‘object’. An object can be thought of as an entity which encapsulates some data and some functions. The data associated with an object are often known as its ‘attributes’, while the functions are often called its ‘methods’. An object can be assigned to a variable, which can then be used to access the data and call the functions.\nA class is a definition for a type of object. It will describe the attributes and methods that an object of this class must contain. Multiple objects of the same class can be generated in a program. Typically, these objects will contain different data. The method definitions are usually common to all objects of a given class, but are always called in the context of a particular object, whose data they will likely act upon.\nTo illustrate these, we will define a class that describes a 3D vector. Note that this is purely an example - if you need a representation of 3D vectors in practise, there are already many good ones available, eg. numpy.ndarray\n\nimport math\n\nclass Vector():\n    n_dimensions = 3\n    def __init__(self, x=0., y=0., z=0.):\n        self._x = x\n        self._y = y\n        self._z = z\n    \n    def __str__(self):\n        return \"x:{0} y:{1} z:{2} norm:{3}\".format(self._x, self._y, self._z, self.magnitude())\n    \n    def x(self):\n        return self._x\n\n    def y(self):\n        return self._y\n    \n    def z(self):\n        return self._z\n\n    def set_x(self, x):\n        self._x = x\n        return\n\n    def set_y(self, y):\n        self._y = y\n        return\n    \n    def set_z(self, z):\n        self._z = z\n        return\n\n    def magnitude(self):\n        return math.sqrt(self._x**2 + self._y**2 + self._z**2)\n\nThe class defines an attribute n_dimensions which will be the same for all objects of this type, and is equal to 3.\nThe __init__() method is as special method, sometimes known as the constructor. If it is defined, it will be called when an object is created. We can use this to create and initialise attributes which are unique to the object, here the three components, _x, _y, and _z. These have a leading underscore to indicate the data is internal to the object.\nThe x(), y(), z() methods provide access to the internal data values. The set_x(), set_y(), set_z() methods allow these attributes to be updated. (See Section 2.1.3 for details on why this is needed).\nThe __str__() method is a useful method which Python will call if you try to convert the object to a string type, eg. by calling print() with the object as an argument, as in the example below. Again, the\nAgain, the double-underscore convention is used to indicate that methods are intended for internal use only, and not by the end user of the class. The two methods here are used by other Python code, but you can define your own internal methods if you find it useful.\nThe magnitude() method does what it says on the tin and returns the magnitude of the vector. This method is intended for the end-user, so it doesn’t include the double-underscores.\nNote the self keyword as an argument to all methods. This is used within a method definition to refer to the particular object upon which the method is being called. Eg. when we call v.magnitude() in the example below, self is equal to v.\nNow we can create some vectors and access their components.\n\n# use the constructor to create a vector and set its components\nv = Vector(3., 4., 0.)\nprint(v)\n\n# now change one of the components\nv.set_z(5.)\nprint(v)\n\nprint(v.magnitude())\n\nx:3.0 y:4.0 z:0.0 norm:5.0\nx:3.0 y:4.0 z:5.0 norm:7.0710678118654755\n7.0710678118654755\n\n\n\n\n2.1.2 Operator overloading\nPython defines a large number of “internal” functions, some of which correspond to operators, such as +, -, /, *. For example, the operation a + a result in a call to the __add__() function. We can redefine these functions to change how these operators behave when operating on a particular class. This is known as operator overloading.\nIn the context of our Vector class, we can encode vector algebra in the class methods. In the code below, note that we pass TWO arguments to each of the operator methods add(), sub(), mul(). The first, self, is the LHS of the operator, the second, a, is the RHS. Also note how we create and return a new Vector object for the + and - methods.\n\nclass Vector():\n    n_dimensions = 3\n    def __init__(self, x=0., y=0., z=0.):\n        self._x = x\n        self._y = y\n        self._z = z\n    \n    def __str__(self):\n        return \"x:{0} y:{1} z:{2} norm:{3}\".format(self._x, self._y, self._z, self.magnitude())\n    \n    def x(self):\n        return self._x\n\n    def y(self):\n        return self._y\n    \n    def z(self):\n        return self._z\n\n    def magnitude(self):\n        return math.sqrt(self._x**2 + self._y**2 + self._z**2)\n        \n    def __add__(self, a):\n        return Vector(self._x+a._x, self._y+a._y, self._z+a._z)\n    def __sub__(self, a):\n        return Vector(self._x-a._x, self._y-a._y, self._z-a._z)\n    def __mul__(self, a):\n        return self._x*a._x + self._y*a._y + self._z*a._z\n\nClearly we have made a choice to define * as the dot product. We could have defined it as the cross product.\n\nv = Vector(4., 3., 0.)\nu = Vector(1., 1., 1.)\nprint(u+v)\nprint(u-v)\nprint(u*v)\n\nx:5.0 y:4.0 z:1.0 norm:6.48074069840786\nx:-3.0 y:-2.0 z:1.0 norm:3.7416573867739413\n7.0\n\n\n\n\n2.1.3 Encapsulation\nEncapsulation regards the bundling of data and methods into a class. Typically, good encapsulation will also involve restricting access to some of the data/methods in the class. This ‘data hiding’ prevents the outside world from directly operating on, or modifying, the class data. The only way to interact with an object of the class will be via its methods. This has several benefits :\n\nCode re-use. The class is designed for a well-defined purpose, and can be easily re-used when required, just by creating a new object of that class.\nSimplifying external code. Users of the class don’t need to know about any complexities of its internal implementation.\nIncreased maintainability. Since the interface to the class is fixed by its methods, the internal operations of the class can be modified without affecting users of the class.\nSecurity. Modification of the class data via unauthorized or unintended methods is prevented.\n\nThese benefits are only realised when classes are well designed. The role of the class, and its interfaces (ie. the class methods), need to be clearly defined, and a developer will need to have understood all the realistic ways the class will be used. (Which we sometimes call the “use cases”).\nIn our example, encapsulation is provided by accessing the internal data of the Vector class (ie. the _x, _y, _z) members via the methods (ie. x(), y(), z()). Note that Python does not provide a way to make data private. The leading underscore convention is an indication to the user that these data should not be directly accessed. But there is no way to enforce it. Other languages, such as C++, do provide mechanisms enforce privacy and will produce errors if access of private data is attempted.\n\n\n2.1.4 Inheritance\nInheritance is a mechanism of OO programming whereby a class can be derived from another class, “inheriting” its properties. Here we’ll talk about a child class inheriting attributes and methods from a parent class. Usually, the child class will have some additional attributes or methods that do not belong to the parent class.\nTo illustrate this, we could consider extending the Vector class to represent a force acting on a point, which might be a useful class in describing problems in mechanics. The force vector would be inherited from Vector, and the point would be an additional Vector member.\n\nclass ForceOnPoint(Vector):\n    def __init__(self, x=0., y=0., z=0., rx=0., ry=0., rz=0.):\n        Vector.__init__(self,x, y, z)\n        self._r = Vector(rx, ry, rz)\n\n    def rx(self):\n        return _r.x()\n\n    def ry(self):\n        return _r.y()\n\n    def rz(self):\n        return _r.z()\n\n    def set_rx(self, rx):\n        _r.x = rx\n        return\n\n    def set_ry(self, ry:\n        _r.y = ry\n        return\n\n    def set_rz(self, rz):\n        _r.z = rz\n        return\n\n\n  Cell In[5], line 19\n    def set_ry(self, ry:\n              ^\nSyntaxError: '(' was never closed\n\n\n\n\nNote how the ForceOnPoint defines a new constructor, which calls the Vector constructor and then an additional line to add the new attribute (the point). We have provided methods to access components of the point vector, which themselves use the component access methods of Vector. So if we wanted to change the internal representation of Vector later, we could do that.\nNote that we’ve used the Vector class to define the r attribute that represents the point. This inclusion of a class within a class is known as “composition”. This is an alternative method to inheritance that will incorporate a class within a class. We could extend this to define the ForceOnPoint class using composition and not inheritance :\n\nclass ForceOnPoint(object):\n    def __init__(self, x=0., y=0., z=0., rx=0., ry=0., rz=0.):\n        # the force vector\n        self._f = Vector(x, y, z)\n        # the point vector\n        self._r = Vector(rx, ry, rz)\n\nHere, the new class represents the force and the point using two separate Vector attributes. We would need to define all the access methods for components of the force and the point.\n\n\n2.1.5 Abstraction\nAbstraction refers to defining the interface to a type of object as a set of methods. It is related to encapsulation, since it is another way of hiding internal details or complexity. To fully exploit this concept, we can define a so-called “abstract base class” which contains only method definitions (not implementation) and no attributes. The abstract class cannot be instantiated as an object, because it has no attributes and no methods that can be called - it is simply an interface definition. Sometimes it is referred to as a blueprint for real classes. A child class which inherits from this class must provide implementations for the methods, together with any required attributes. Sometimes the child class is called a “concrete class”, as it can be instantiated.\nTypically, abstract base classes are used when we have a collection of related classes, which we want to have some common features. This is illustrated with a simple example of an abstract base class that describes a vehicle, with several concrete classes that inherit from it.\n\nclass Vehicle(object):\n    def n_wheels(self):\n        pass\n    def motorised(self):\n        pass\n    def mass(self):\n        pass\n\nclass Car(Vehicle):\n    def __init__(self, m):\n        self._m = m\n    def n_wheels(self):\n        return 4\n    def motorised(self):\n        return True\n    def mass(self):\n        return _m\n\nclass Bicycle(Vehicle):\n    def __init__(self, m):\n        self._m = m\n    def n_wheels(self):\n        return 2\n    def motorised(self):\n        return False\n    def mass(self):\n        return _m\n\nNote the use of the pass keyword in the abstract base class to complete the method definition without providing any functionality.\n(Note that Python does not support ‘true’ abstract base classes natively. There are several reasons the Vehicle class in the example above is not truly abstract, which are beyond the scope of this unit. A simple reason is that you can create an instance of the Vehicle class without errors. For the purists, true abstraction is supported via the abc module).\n\n\n2.1.6 Polymorphism\nPolymorphism means “many forms”, and refers to an ability to treat objects of many different types as if they are of a single common type. In OO programming, this is usually achieved through inheritance and abstraction. For example, in the previous example, we can treat objects of type Car and Bicycle as if they are both just of type Vehicle. If we’re only interested in computing the mass of the Vehicles in the list, it doesn’t matter that some are Cars and some are Bicycles - we just call the mass() method on each one.\nPython supports polymorphism even without use of OO/inheritance/abstraction. The Python interpreter will attempt to call a method on object; if the method exists then it will run, and if it doesn’t an error will be generated. So it doesn’t matter if two objects have the same method via inheritance, or simply because we gave them both a method with the same name. This is known as “duck typing” - if an object quacks like a duck, then Python will treat it as a duck. (Python will also treat the same object as a horse, provided it finds the methods expected of a horse).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Advanced Python Programming</span>"
    ]
  },
  {
    "objectID": "adv-python.html#functional-programming-lambdas",
    "href": "adv-python.html#functional-programming-lambdas",
    "title": "2  Advanced Python Programming",
    "section": "2.2 Functional Programming & Lambdas",
    "text": "2.2 Functional Programming & Lambdas\nFunctional programming is another programming paradigm. In some respects it is quite the opposite of OO programming. Objects intrinsically have some ‘state’ associated with them, due to their attributes (internal data). So the result of a function call in OO programming (ie. a method) will depend on both the arguments passed to the function, and the state of the object upon which the method is called.\nConversely, in functional programming, computation is expressed in terms of functions which have no state. This paradigm is based on a formal system called λ-calculus, which has been shown to be Turing-complete, ie. it can simulate any Turing machine, or it can implement any computer algorithm. There are several languages (eg. Haskell, Erlang) which are designed to enforce functional programming. It is possible to write fully functional programs in Python, using lambda functions.\nDeveloping full functional programs is beyond the scope of this unit, but it should be noted that functional programming can be extremely useful in distributed computing. Because functions have no state, all the data they need to operate is passed as an argument. This means it doesn’t really matter where the function runs. Provided there is a network connection to the computer which is running the function, the arguments to the function call and the returned value can be communicated. Conversely, in OO programming, methods need to be run in the same location as the obect attributes they are associated with. This means OO programs can be harder to run in a distributed fashion than functional ones.\nA full introduction to functional programming is beyond the scope of the unit, but below we introduce the syntax for lambda functions, and some particular cases where they can be useful.\n\n2.2.1 Lambda Functions\nIn Python, lambda functions are created using the lambda keyword, eg.: lambda: x, x**2 returns an “anonymous” function that takes one argument, and returns a value that depends only on the argument (in this case, the square). Compare this to regular functions, which are declared with def and are named :\n\ndef function(x):\n    return 0.5*x**2 + x + 1\nfunction(4)\n\n13.0\n\n\nThe equivalent using a lambda function would be :\n\n(lambda x: 0.5*x**2 + x + 1)(4)\n\n13.0\n\n\n\n\n2.2.2 Anonymous functions\nLambdas can be useful when implementing mathematical expressions, and when working with functions that expect other functions as arguments. Eg. suppose I want to calculate \\(\\int_1^4 e^{−x}\\). I can do this in one line with a lambda function :\n\nimport scipy.integrate\nimport math\n\nscipy.integrate.quad(lambda x: math.exp(-1*x), 1., 4.)\n\n(0.34956380228270817, 3.880937818697785e-15)\n\n\n\n\n2.2.3 Map-Reduce with Lists\nAnother area where lambda functions are useful is in the “map-reduce” paradigm. The idea here is that when processing large amounts of data, you want to do as much processing in parallel as possible. A given algorithm is divided into a parallel part (map), and a non-parallel part (reduce). When processing huge datasets, the map will be running in parallel on multiple machines. In Python, the map() function takes a function and a list as arguments. The function is applied to each element in the list, and then a list of the results is returned. Eg. to calculate the expression below for for n = 1000 : \\[\n\\sum_{i=1}^n i^2\n\\]\n\nimport numpy as np\nfrom functools import reduce\n\nx = np.arange(0,1000)\n\nsquares = list(map((lambda x: x**2), x))\nsum = reduce((lambda x, y: x+y), squares)\n\nprint(sum)\n\n332833500\n\n\nAnother useful function when working with lists is filter(). This takes a function and a list, applies the function to each element, and retains that element if the function returns True. For example, you can try writing a function that accepts the odd values in a list of integers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Advanced Python Programming</span>"
    ]
  },
  {
    "objectID": "adv-python.html#exceptions",
    "href": "adv-python.html#exceptions",
    "title": "2  Advanced Python Programming",
    "section": "2.3 Exceptions",
    "text": "2.3 Exceptions\nThis section is a short introduction to “Exceptions”, an error handling technique commonly used in modern programming languages. It is the standard way to deal with errors in Python and both numpy and scipy use them. Raising exceptions in your own functions, and knowing how to deal with exceptions raised by other code, will help you write more robust code. It’s worth noting that exceptions are not the only way of handling errors. Code using the simple method below will be entirely adequate in many cases. You will need to decide what is appropriate for your particular case. However, understanding exceptions is important when using libbraries (such as SciPy) which raise exceptions when they encounter errors. First we’ll look at error handling in a simple case, without using exceptions.\n\n2.3.1 Simple Error Handling\nSuppose I have a function that works for positive number, but I know it will fail if given a negative input. Rather than let the code crash, or (worse) return incorrect results, we can detect this problem and do something sensible. One option might be to test the argument given to the function, print an error message, and return a default value (NaN, in this case) :\n\nimport math\nimport numpy as np\ndef mySqrt(x):\n    if x&lt;0:\n        print(\"Input must be positive.\")\n        return np.nan\n    return math.sqrt(x)\n\nLet’s test this with a couple of examples :\n\nprint(mySqrt(4))\nprint(mySqrt(-3))\n\n2.0\nInput must be positive.\nnan\n\n\nThis method for dealing with errors is better than nothing, but it has limitations. In particular : 1. Returning a default value when the input is invalid may cause further knock-on problems 2. We have no way of knowing what caused our function to be called with an invalid argument\nWe can avoid 1. by eg. halting execution of the program after printing the error message, eg. by calling sys.exit(). However, this is quite extreme and maybe not appropriate for all cases. And even if halting execution is the only option - we still have no way of dealing with point 2.\n\n\n2.3.2 Error Handling with Exceptions\nExceptions are the standard way to deal with errors in Python, and both numpy and scipy use them. Raising exceptions in your own functions, and knowing how to deal with exceptions raised by other code, can help you write more robust code. However, exceptions are just an error han- dling technique - you will still need to analyse where errors can occur in your code and make suitable provisions for handling them. The key feature of exceptions is that they allow the programmer to decide where in the code is the appropriate place to take action. In the example here, this might be in the function that calls mySqrt(). But in other circumstances, it might be in the function that calls that function. When an exception is generated, it is communciated back through all function calls until a block of code ‘catches’ the exception(and takes some action. Ultimately, if nothing handles the exception, the program will stop. Here is how we would handle this error using an exception :\n\ndef mySqrt(x):\n    if x&lt;0:\n        raise Exception(\"Negative input\")\n    return math.sqrt(x)\n\nprint(mySqrt(4))\nprint(mySqrt(-4))\n\n2.0\n\n\n\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\nCell In[14], line 7\n      4     return math.sqrt(x)\n      6 print(mySqrt(4))\n----&gt; 7 print(mySqrt(-4))\n\nCell In[14], line 3, in mySqrt(x)\n      1 def mySqrt(x):\n      2     if x&lt;0:\n----&gt; 3         raise Exception(\"Negative input\")\n      4     return math.sqrt(x)\n\nException: Negative input\n\n\n\nYou might have seen this kind of print out when debugging code. The “Traceback” lists the function calls that led to the exception being raised.\n\n\n2.3.3 Catching Exceptions\nRaising an exception is only half of the process. The other half is “catching” them. Let’s say we have a function that calls mySqrt() but it knows what to do if the exception is raised. We can use a “try-except” (also known as “try-catch”, from the corresponding C++ keywords) block to catch that exception and take the correct course of action.\n\n    try:\n        y = mySqrt(x)\n    except Exception:\n        y = 1j * mySqrt(abs(x))\nreturn y\n\nprint(mySqrtComplex(16))\nprint(mySqrtComplex(-16))\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[15], line 2\n      1 try:\n----&gt; 2     y = mySqrt(x)\n      3 except Exception:\n\nCell In[14], line 2, in mySqrt(x)\n      1 def mySqrt(x):\n----&gt; 2     if x&lt;0:\n      3         raise Exception(\"Negative input\")\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[15], line 4\n      2     y = mySqrt(x)\n      3 except Exception:\n----&gt; 4     y = 1j * mySqrt(abs(x))\n      5 return y\n      7 print(mySqrtComplex(16))\n\nCell In[14], line 2, in mySqrt(x)\n      1 def mySqrt(x):\n----&gt; 2     if x&lt;0:\n      3         raise Exception(\"Negative input\")\n      4     return math.sqrt(x)\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\n\n\n\n\n2.3.4 Handling Other Exceptions\nYou might find you can get away without raising any exceptions in your code. However, scipy and numpy will raise exceptions, and knowing how to handle them can be useful. For example, a number of linear algebra routines in scipy.linalg cannot proceed if given a singular matrix. In this case, they will raise a numpy.linalg.LinAlgError exception : https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.LinAlgError.html#numpy.linalg.LinAlg As well as the built-in Exception, Python allows us to define dedicated exception types, like this one. This allows exception handling code to distinguish different classes of error condition, which is useful when deciding what to do. An example of how to catch this kind of exception is below.\n\nimport scipy.linalg\nm = np.zeros((2,2))\nprint(m)\ntry:\n    scipy.linalg.inv(m)\nexcept scipy.linalg.LinAlgError as err:\n    print(\"Caught an exception :\", err)\n\n[[0. 0.]\n [0. 0.]]\nCaught an exception : singular matrix",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Advanced Python Programming</span>"
    ]
  },
  {
    "objectID": "linear-algebra.html",
    "href": "linear-algebra.html",
    "title": "3  Linear Algebra",
    "section": "",
    "text": "3.1 Basic Matrix Operations\nPython/numpy/scipy provide a range of options for achieving basic matrix operations. You will need to take a little care to ensure that your code implements the operations you intend it to. This is largely due to the fact that some operators/functions will change their behaviour depending on the input you provide. This section includes some recommendations for simple linear algebra, which should ensure your code behaves as desired.\nMatrices can be implemented as a 2D np.ndarray. Basic matrix arithmetic can then be performed using standard operators +,- and @. You can also use np.matmul() for matrix multiplication. Numpy will also perform matrix multiplication with np.dot(), but this is not recommended if you can use @ or np.matmul().\nimport numpy as np\n\nA = np.array([[1, 2, 3,],[4, 5, 6],[1, 0, 0]])\nB = np.array([[1, 0, 0,],[0, 1, 0],[0, 0, 1]])\n\n# addition\nprint(A + B)\n\n# subtraction\nprint(A - B)\n\n# scalar multiplication\nprint(3*B)\n\n# matrix multiplication\nprint(np.matmul(A, B))\nprint(A @ B)\n\n[[2 2 3]\n [4 6 6]\n [1 0 1]]\n[[ 0  2  3]\n [ 4  4  6]\n [ 1  0 -1]]\n[[3 0 0]\n [0 3 0]\n [0 0 3]]\n[[1 2 3]\n [4 5 6]\n [1 0 0]]\n[[1 2 3]\n [4 5 6]\n [1 0 0]]\nVectors can be implemented as 1D arrays, or as 2D arrays. A 1D array will be interpreted as row or column vector depending on the context in which it is used. Use of 2D arrays allows you to specify row or column form. This can be useful, since np.matmul() or @ will throw an exception if you accidentally try to perform an illegal matrix operation.\nv  = np.array([1,2,3])\nvr = np.array([[1,2,3]])\nvc = np.array([[1],[2],[3]])\n\n# two options for matrix * vector\nprint(A@v)\nprint(A@vc)\n\n# two options for vector * matrix\nprint(v@A)\nprint(vr@A)\n\n# this is not a valid matrix multiplication !\nprint(A@vr)\n\n[14 32  1]\n[[14]\n [32]\n [ 1]]\n[12 12 15]\n[[12 12 15]]\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[2], line 14\n     11 print(vr@A)\n     13 # this is not a valid matrix multiplication !\n---&gt; 14 print(A@vr)\n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 1 is different from 3)\nNumpy will also provide the usual forms of vector product via np.vdot(), np.cross(), np.inner() and np.outer(). Again, np.dot() will provide a vector dot product, but is not recommended if you can use vdot().\nOther useful matrix operations are provided by numpy, such as : - np.transpose() (also available via ndarray.T) - np.norm() - np.trace()\nFor further information, look at the reference pages : https://numpy.org/doc/stable/reference/routines.array-manipulation.html https://numpy.org/doc/stable/reference/routines.linalg.html\nFinally, scipy.linalg provides some additional basic operations such as the determinant and the inverse.\nimport numpy as np\nimport scipy.linalg as linalg\n\nA = np.array([[1, 2, 3,],[4, 5, 6],[1, 0, 0]])\ndetA = linalg.det(A)\nprint(detA)\n\ninvA = linalg.inv(A)\nprint(invA)\n\n-3.0\n[[-0.          0.          1.        ]\n [-2.          1.         -2.        ]\n [ 1.66666667 -0.66666667  1.        ]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "linear-algebra.html#simultaneous-equations",
    "href": "linear-algebra.html#simultaneous-equations",
    "title": "3  Linear Algebra",
    "section": "3.2 Simultaneous Equations",
    "text": "3.2 Simultaneous Equations\nMany problems in physics require solving simultaneous equations. When these become large and complex, numerical routines are required.\nA set of simultaneous equations can always be written in matrix form, for example, two equations in two unknowns (\\(x_1\\) and \\(x_2\\))\n\\[\n\\begin{aligned}\nax_1 + bx_2 &= y_1 \\\\\ncx_1 + dx_2 &= y_2\n\\end{aligned}\n\\tag{3.1}\\]\ncan be rewritten as\n\\[\n\\left(\\begin{array}{cc} a & b \\\\ c & d\\end{array}\\right)\n\\left(\\begin{array}{c} x_1 \\\\ x_2 \\end{array}\\right) =\n\\left(\\begin{array}{c} y_1 \\\\ y_2 \\end{array}\\right)\n\\tag{3.2}\\]\nAn arbitrary set of equations is\n\\[Ax = y \\tag{3.3}\\]\nwhere A is the matrix of coefficients, x is the vector of unknown variables \\(x_1\\), \\(x_2\\), … and y is the known vector of constants.\n\n3.2.1 Inverse Matrix\nOne way to solve the above equation is to multiply both sides by the inverse of A:\n\\[A^{-1} A x = A^{-1} y \\tag{3.4}\\]\ngiving :\n\\[x = A^{-1} y \\tag{3.5}\\]\nThis is demonstrated in the example below for a simple test case :\n\\[\n\\left(\\begin{array}{ccc} 1 & 2 & 2 \\\\\n                        3 & 1 & 6 \\\\\n                        0 & 2 & 2\\end{array}\\right)\n\\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right) =\n\\left(\\begin{array}{c} 2 \\\\ 7 \\\\ 1\\end{array}\\right)\n\\tag{3.6}\\]\n\nimport numpy as np\nimport scipy.linalg as linalg\n\ndef solve_inv(a,y):\n    x = linalg.inv(a) @ y\n    return x\n\na = np.array([[1, 2, 2,],[3, 1, 6],[0, 2, 2]])\ny = np.array([[2], [7], [1]])\nprint(a)\nprint(y)\n\nx = solve_inv(a,y)\nprint(x)\n\n[[1 2 2]\n [3 1 6]\n [0 2 2]]\n[[2]\n [7]\n [1]]\n[[ 1. ]\n [-0.2]\n [ 0.7]]\n\n\nIs this the solution? We can easily check by inserting the solution into the original equation.\n\nprint(a @ x)\n\n[[2.]\n [7.]\n [1.]]\n\n\nWhich is indeed equal to our y above. This kind of test is known as a ‘closure test’ and will be used frequently throughout this unit to verify our code.\nBefore using this method for solving simultaneous equations, though, we should understand how scipy.linalg.inv finds the matrix inverse. Unfortunately, this is tricky to understand from the reference page, (scipy.linalg.inv). But, if you examine the source code for this function you’ll see that it uses a function called DGETRI. This is defined in the LAPACK library, and its reference page is here. As you can see this routine uses LU decomposition to find the inverse! It doesn’t make sense, therefore, to find a matrix inverse simply to solve a simultaneous equation, and using LU decomposition directly will involve fewer operations. However, there are exceptions when dealing with many simultaneous equations. For example, suppose you have a sequence of problems which all feature the same matrix \\(A\\), but have different RHS \\(y\\). In this case it would be efficient to invert \\(A\\) once, then multiply by \\(y\\) to solve each problem, since multiplication involves fewer operations than LU decomposition.\nAs an aside, LAPACK is a linear algebra library written in FORTRAN - which remains one of the most efficient languages for writing numerical methods - and most routines in scipy.linalg basically provide a Python interface to this library.\n\n\n3.2.2 Gaussian Elimination\nSome sets of simultaneous equations are easy to solve. For example :\n\\[\n\\begin{aligned}\na x_1 &= y_1 \\\\\nb x_2 &= y_2 \\\\\nc x_3 &= y_3\n\\end{aligned}\n\\tag{3.7}\\] This can be written in what is known as row echelon form :\n\\[\n\\left(\\begin{array}{ccc} a & 0 & 0 \\\\\n                        0 & b & 0 \\\\\n                        0 & 0 & c\\end{array}\\right)\n\\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right) =\n\\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3\\end{array}\\right)\n\\tag{3.8}\\]\nAnd then reduced row echelon form :\n\\[\n\\left(\\begin{array}{ccc} 1 & 0 & 0 \\\\\n                        0 & 1 & 0 \\\\\n                        0 & 0 & 1\\end{array}\\right)\n\\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right) =\n\\left(\\begin{array}{c} y_1/a \\\\ y_2/b \\\\ y_3/c\\end{array}\\right)\n\\tag{3.9}\\]\nGauss-Jordan elimination is a process which reduces any linear equation set to this form. It can be shown that the reduced row echelon form is unique, and therefore independent of the order of operations which are used to find it. The technique is illustrated using the example problem from earlier. It’s convenient to use the ‘augmented’ matrix, which includes the right-hand side. \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 2 & 2 & 2 \\\\\n    3 & 1 & 6 & 7 \\\\\n    0 & 2 & 2 & 1\n\\end{array}\\right)\n\\tag{3.10}\\]\nThen we apply simple operations until we obtain the equation in row echelon form. These operations include:\n\nMultiply a row by a constant\nSwap two rows\nSum two rows in a linear combination\n\n(Hopefully, these sound reasonably familiar - we are just formalising techniques you will have used before)\nReplace \\(R_1\\) (row 1) with \\(R_1 - R_3\\) : \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 0 & 0 & 1 \\\\\n    3 & 1 & 6 & 7 \\\\\n    0 & 2 & 2 & 1\n\\end{array}\\right)\n\\tag{3.11}\\]\nReplace \\(R_2\\) with \\(R_2 - 3R_1\\) : \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 0 & 0 & 1 \\\\\n    0 & 1 & 6 & 4 \\\\\n    0 & 2 & 2 & 1\n\\end{array}\\right)\n\\tag{3.12}\\]\nReplace \\(R_2\\) with \\(R_2 - \\frac{2}{5}R_2\\) : \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 0 & 0 & 1 \\\\\n    0 & -5 & 0 & 1 \\\\\n    0 & 0 & 2 & \\frac{7}{5}\n\\end{array}\\right)\n\\tag{3.13}\\]\nAnd then finally for reduced row echelon form, replace \\(R_2\\) with \\(\\frac{-1}{5}R_2\\) and \\(R_3\\) with \\(\\frac{1}{2}R_3\\) \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 0 & 0 & 1 \\\\\n    0 & 1 & 0 & -\\frac{1}{5} \\\\\n    0 & 0 & 1 & \\frac{7}{10}\n\\end{array}\\right)\n\\tag{3.14}\\]\nSo the solution is : \\[\n\\begin{aligned}\nx_1 &= 1 \\\\\nx_2 &= -\\frac{1}{5} \\\\\nx_3 &= \\frac{7}{10}\n\\end{aligned}\n\\tag{3.15}\\]\n\n\n3.2.3 LU Decomposition\nMatrix decomposition techniques involve factorising a general matrix into a product of several matrices. LU decomposition involves writing the general matrix, \\(A\\), as the product of two triangular matrices, \\(L\\) and \\(U\\).\n\\[\nA=\n\\left(\\begin{array}{ccc} a_{11} & a_{12} & a_{13} \\\\\n                         a_{21} & a_{22} & a_{23} \\\\\n                         a_{31} & a_{32} & a_{33}\n\\end{array}\\right)\n\\tag{3.16}\\]\n\\[\nA=LU=\n\\left(\\begin{array}{ccc} 1      & 0      & 0 \\\\\n                         l_{21} & 1      & 0 \\\\\n                         l_{31} & l_{32} & 1\n\\end{array}\\right)\n\\left(\\begin{array}{ccc} u_{11} & u_{12} & u_{13} \\\\\n                         0      & u_{22} & u_{23} \\\\\n                         0      & 0      & u_{33}\n\\end{array}\\right)\n\\tag{3.17}\\]\nWe can use LU decomposition to solve matrix equations since it allows us to write the equation \\[Ax = y\\] as \\(L(Ux)=y\\). This can then be written as two equations \\(Lc=y\\) and \\(Ux=c\\), which are trivially solved, first for \\(c\\), and then for \\(x\\).\nThe matrices \\(L\\) and \\(U\\) can be found using the operations described above for Gaussian elimination. There are several algorithmic formulations that define the sequence of operations. Scipy provides an LU decomposition routine: scipy.linalg.lu(). Note that this performs a variation on the LU decomposition described above, since it also computes a permutation matrix \\(P\\), such that\n\\[PA = LU \\tag{3.18}\\]\nScipy also provides a simple function to obtain the solutions to a matrix equation. scipy.linalg.lu_solve() expects the \\(L\\), \\(U\\) and \\(P\\) matrices as arguments, as shown in the example below.\n\nscipy.linalg.lu\nscipy.linalg.lu_solve\n\n\ndef solve_lu(a,y):\n    lu, piv = linalg.lu_factor(a)\n    x = linalg.lu_solve((lu, piv), y)\n    return x\n\nprint(solve_lu(a,y))\n\n[[ 1. ]\n [-0.2]\n [ 0.7]]\n\n\nNote that the general purpose solvers provided by both numpy and scipy both utilise LU decomposition :\n\nnumpy.linalg.solve\nscipy.linalg.solve\n\n\n\n3.2.4 SVD Decomposition\nLU decomposition will find an exact solution to the matrix equation in a wide variety of cases. However, a solution may not exist, or there may be infinite solutions. In such cases, the Singular Value Decomposition may be of use.\nFor an \\(m \\times n\\) matrix \\(A\\), the singular values, \\(\\sigma\\) are given by the solutions to\n\\[\n\\begin{aligned}\nAv &= \\sigma u \\\\\nA^Tu &=\\sigma v\n\\end{aligned}\n\\tag{3.19}\\]\nwhere \\(u\\) and \\(v\\) are two non-zero vectors. These equations are closely related to the eigenvalue equation. Indeed, the singular values are also the square roots of the eigenvalues of \\(A^TA\\).\nThe singular value decomposition of \\(A\\) is\n\\[A = U\\Sigma V^T \\tag{3.20}\\]\nwhere \\(U\\) and \\(V\\) are orthonormal matrices, and \\(\\Sigma\\) is a matrix with the singular values on its leading diagonal, and zero elsewhere.\nThe SVD decomposition allows use to compute the pseudo-inverse of \\(A\\), which is given by :\n\\[A^\\dagger = V \\Sigma^\\dagger U^T \\tag{3.21}\\]\nwhere \\(\\Sigma^\\dagger\\) is the pseudo-inverse of \\(\\Sigma\\) and is obtained by transposing \\(\\Sigma\\) and replacing each non-zero element with it’s reciprocal.\nThe pseudoinverse (also known as the Moore-Penrose inverse) can always be computed, even when \\(A\\) is singular, ie. when when \\(\\frac{1}{|A|}=0\\) and the inverse cannot be found.\nIn the context of solving a matrix equation \\(Ax=y\\), the product of pseudoinverse and the RHS, (i.e. \\(\\bar{X}=A^\\dagger y\\)) has various properties. When A is non-singular, \\(\\bar{x}\\) gives the solution to \\(Ax=y\\). When \\(A\\) is singular, \\(\\bar{x}\\) is a least squares approximation to the nearest solution. When \\(Ax=y\\) has a space of solutions (equivalent to a set of simultaneous equations with degeneracy), then \\(\\bar{x}\\) is a vector which describes this space.\nSVD decomposition is available in Scipy using scipy.linalg.svd(). For further information, see scipy.linalg.svd. Note that, unlike LU decomposition, no solve() function is supplied, and instead we must write some code to calculate \\(\\bar{x}\\).\n\ndef solve_svd(a,y):\n    u, s, v = linalg.svd(a)\n    x = v.T @ np.diag(1/s) @ u.T @ y\n    return x\n\nprint(solve_svd(a,y))\n\n[[ 1. ]\n [-0.2]\n [ 0.7]]\n\n\nAlternatively, a function to compute the pseudoinverse directly is provided, scipy.linalg.pinv.\n\nprint( linalg.pinv(a) @ y)\n\n[[ 1. ]\n [-0.2]\n [ 0.7]]\n\n\n\n\n3.2.5 Physics Example\nHere we illustrate the use of simultaneous equation solvers in a familiar context - the use of Kirchoff’s laws and Ohm’s law to understand resistor networks. Typically, analysis of a resistor network will involve solving simultaneous equations, to calculate voltage and current at the desired points in the network. Consider the electronic circuit shown in the diagram.\n\n\n\nFigure 1 - Example resistor network.\n\n\nWhere : \\(V_1 = 12V\\), \\(V_2 = 12V\\), \\(R_1 = 3 \\Omega\\), \\(R_2 = 3 \\Omega\\), \\(R_3 = 10 \\Omega\\), \\(R_4 = 2 \\Omega\\), \\(R_5 = 2 \\Omega\\).\nBy identifying the three current loops indicated, we can use Kirchoff’s loop rule and Ohm’s law to write : \\[\n\\begin{aligned}\nV_1 &= I_1 R_1 + (I_1 - I_2) R_2 \\\\\n0   &= (I_2 - I_1) R_2 + I_2 R_3 + (I_2 - I_3) R_4 \\\\\n-V_2 &= (I_3 - I_2) R_4 + I_3 R_5\n\\end{aligned}\n\\tag{3.22}\\]\nSo we have a set of simultaneous equations, which we can write as a matrix equation :\n\\[\n\\begin{pmatrix}\nR_1+R_2 & -R_2 & 0 \\\\\n-R_2 & R_2+R_3+R_4 & -R_4 \\\\\n0  & -R_4 & R_4+R_5 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nI_1 \\\\\nI_2 \\\\\nI_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nV_1 \\\\\n0 \\\\\n-V_2\n\\end{pmatrix}\n\\tag{3.23}\\]\nSolving this matrix equation will provide the current at all points in the circuit. This method is known as “mesh analysis” of circuits.\nWe can write a simple function that, given the voltage and resistor values, will return the currents :\n\ndef meshAnalysis(v1, v2, r1, r2, r3, r4, r5):\n    m = np.array ([[r1+r2, -r2, 0],[-r2,r2+r3+r4,-r4],[0,-r4,r4+r5]])\n    v = np.array([[v1],[0],[-v2]])\n    i = linalg.solve(m,v)\n    return i\n\nWhich, for the values given, will return the three currents :\n\ni = meshAnalysis(12, 12, 3, 3, 10, 2, 2)\n\nprint(i)\n\n[[ 2.]\n [ 0.]\n [-3.]]\n\n\nOr we could calculate, for example, how \\(I_2\\) will vary as a function of \\(R_4\\), with all other values fixed :\n\nr4s = np.linspace(0.1, 5.0, 100)\ni2s = np.empty(len(r4s))\n\nfor j,r4 in enumerate(r4s):\n    i     = meshAnalysis(12, 12, 3, 3, 10, r4, 2)\n    i2s[j] = i[1][0]\n\nimport matplotlib.pyplot as plt\n\nplt.plot(r4s, i2s)\nplt.ylabel('$I_2$ (A)')\nplt.xlabel('$R_4$ (ohm)')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "linear-algebra.html#eigenproblems",
    "href": "linear-algebra.html#eigenproblems",
    "title": "3  Linear Algebra",
    "section": "3.3 Eigenproblems",
    "text": "3.3 Eigenproblems\nA square NxN matrix \\(A\\), has eigenvector \\(u\\) and eigenvalue \\(\\lambda\\) that satisfy :\n\\[(A - \\lambda I)u = 0 \\tag{3.24}\\]\nSolving eigenproblems is closely related to finding the roots of polynomials. You may be familiar with one method for finding eigenvalues, which is to find the roots of the N-th degree polynomial found by expanding :\n\\[p(t) = \\det{|A - t I|} = 0 \\tag{3.25}\\]\nMany eigenproblem solving routines are provided by SciPy. In particular, scipy.linalg.eig(A) will return a tuple containing the eigenvalues and eigenvectors of A. If only the eigenvalues are required, scipy.linalg.eigenvals(A) can be used.\nCare should be taken when using scipy.linalg.eig, since it will find “left” and “right” eigenvectors, as specified, which are the solutions to \\(v A = \\lambda v\\) and \\(A v = \\lambda v\\) respectively.\nFor further details see scipy.linalg.eig\nAdditional routines include :\n\nscipy.linalg.eigh (for Hermitian matrices)\nscipy.linalg.eig_banded (for banded matrices)\nscipy.sparse.linalg.eigs (for sparse, square symmetric matrices)\n\n\n3.3.1 Simple example\nWe can test these sovlers using the matrix :\n\\[A =\n\\pmatrix{\n-2 & -4 & 2 \\\\\n-2 &  1 & 2 \\\\\n4  &  2 & 5}\n\\tag{3.26}\\]\nfor which the eigenvalues are \\(\\lambda^{(0)}=6\\), \\(\\lambda^{(1)}=-5\\), \\(\\lambda^{(2)}=3\\).\nNote that the algorithms discussed here will all find unit eigenvectors. The eigenvector corresponding to \\(\\lambda^{(0)}\\) is then :\n\\[\\hat{u}^{(0)}=\\pmatrix{\\frac{1}{\\sqrt{293}} \\\\\n\\frac{6}{\\sqrt{293}} \\\\\n\\frac{16}{\\sqrt{293}}\n}\n=\n\\pmatrix{0.058 \\\\\n0.351 \\\\\n0.935} \\tag{3.27}\\]\nwith numerical values given to 3 decimal places on the RHS.\n\nimport numpy as np\nimport scipy.linalg as linalg\n\nm = np.array([[-2,-4,2],[-2,1,2],[4,2,5]])\n\n# set seed for repeatability\nnp.random.seed(2)\n\n# run the algorithm\nmus, vs = linalg.eig(m)\n\n# print results\nnp.set_printoptions(precision=3)\nfor i in range(3):\n    print(\"Eigenvalue/vector : {:.1f} {}\".format(mus[i], vs.T[i]))\n\nEigenvalue/vector : -5.0+0.0j [ 0.816  0.408 -0.408]\nEigenvalue/vector : 3.0+0.0j [ 0.535 -0.802 -0.267]\nEigenvalue/vector : 6.0+0.0j [0.058 0.351 0.935]\n\n\nWhich includes the solution expected.\nNote that we have transposed the array of eigenvectors returned by linalg.eig(). This is a feature of the function, as described in the reference manual : “The normalized left eigenvector corresponding to the eigenvalue w[i] is the column vl[:,i]”.\n\n\n3.3.2 Physics Example\nIn this section, we illustrate the use of eigenvalue solvers in finding stable solutions of the coupled system of oscillators shown below.\n\n\n\nFigure 2 - Coupled oscillators, masses on springs.\n\n\nIf the displacement of the \\(i\\)th mass from its equilibrium position is denoted as \\(x_i\\), the force on the mass is given by the tension in the two springs as :\n\\[F_i = −k(x_i − x_{i−1}) + k(x_{i+1} − x_i) = −k(2x_i − x_{i−1} − x_{i+1}) \\tag{3.28}\\]\nWe can assume that there are normal mode solutions, i.e. solutions of the form \\(x_i = z_i e^{i\\omega t}\\) in which all masses oscillate with the same frequency \\(\\omega\\) but with unknown phasors \\(z_i\\). Then the above equation becomes :\n\\[F_i = m\\ddot{x}_i = −m\\omega^2x_i = −k(2x_i − x_{i−1} − x_{i+1}) \\tag{3.29}\\]\nThis is one row of a matrix equation describing the entire system :\n\\[m\\omega^2x_i \\left(\\begin{array}{c} \\vdots \\\\ \\\\ x_i \\\\ \\\\ \\vdots \\end{array}\\right) =\n\\left(\\begin{array}{ccccccc} & & & \\vdots & & & \\\\ \\cdots & 0 & -1 & 2 & -1 & 0 & \\cdots \\\\ & & & \\vdots & & & \\end{array}\\right)\n\\left(\\begin{array}{c} \\vdots \\\\ x_{i-1} \\\\ x_i \\\\ x_{i+1} \\\\ \\vdots \\end{array}\\right)\n\\tag{3.30}\\]\nThis example is a typical eigenvalue problem, in that many of the matrix elements are zero, which can greatly simplify the computational challenge and make even large systems solvable. The matrix is symmetric, which means it is suitable for solving with our eigenproblem solving function above, or one of the solvers from scipy.linalg.\n\nm = np.array([[2, -1,  0,  0,  0,  0,  0],\n              [-1, 2, -1,  0,  0,  0,  0],\n              [0, -1,  2, -1,  0,  0,  0],\n              [0,  0, -1,  2, -1,  0,  0],\n              [0,  0,  0, -1,  2, -1,  0],\n              [0,  0,  0,  0, -1,  2, -1],\n              [0,  0,  0,  0,  0, -1,  2]])\n\nmus, vs = linalg.eig(m)\n\nThe eigenvalue associated with each mode gives the frequency, while the (complex) eigenvector provides the magnitude and phase of oscillation for each mass. We can plot the displacement of each mass as a function of time for each mode.\n\nimport numpy as np\nimport scipy.linalg as linalg\nimport matplotlib.pyplot as plt\n\n# a function to calculate the (real) displacement from complex phase\ndef disp(zi, omega, t):\n    return np.real(zi * np.exp(1j * omega * t))\n\n# set up some pretty colours for plotting\ncm  = plt.cm.viridis\ncol = [cm(int(x*cm.N/7)) for x in range(7)]\n\n# time period\nts = np.arange(0,40, 0.001)\n\n# loop over eigenmodes\nfor i in range(7):\n    \n    print(\"Mode       : \",i)\n    print(\"Eigenvalue : \", mus[i])\n\n    fig=plt.figure(figsize=(16, 4))\n\n    xs = []\n    \n    # loop over masses\n    for j in range(7):\n        \n        # get the displacement, and add an offset to separate out each line\n        offset = (2*j)-6\n        \n        # create displacement values from function using eigenvectors and eigenvalues\n        xs     = disp(vs.T[i][j], mus[i], ts) + offset\n        \n        # plot displacement\n        plt.plot(ts, xs, color=col[j])\n        \n        # plot central position to guide the eye\n        plt.plot([0, 40], [offset, offset], color=col[j], linestyle='dotted') \n\n    plt.xlabel(\"t\")\n    plt.show()\n\nMode       :  0\nEigenvalue :  (3.847759065022569+0j)\n\n\n\n\n\n\n\n\n\nMode       :  1\nEigenvalue :  (3.4142135623730927+0j)\n\n\n\n\n\n\n\n\n\nMode       :  2\nEigenvalue :  (2.765366864730179+0j)\n\n\n\n\n\n\n\n\n\nMode       :  3\nEigenvalue :  (1.9999999999999984+0j)\n\n\n\n\n\n\n\n\n\nMode       :  4\nEigenvalue :  (0.15224093497742724+0j)\n\n\n\n\n\n\n\n\n\nMode       :  5\nEigenvalue :  (0.585786437626905+0j)\n\n\n\n\n\n\n\n\n\nMode       :  6\nEigenvalue :  (1.2346331352698205+0j)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "monte-carlo.html",
    "href": "monte-carlo.html",
    "title": "4  Monte Carlo Methods",
    "section": "",
    "text": "4.1 Pseudo Random Number Sampling\nWe will frequently want to generate random numbers according to a particular probability distribution, or in the jargon ‘sampling’ from the given distribution. There are two basic methods for achieving this; an analytical method, and the accept/reject method.\nIn the below, we assume we are provided with a random number generator that returns pseudo-random numbers with a uniform probability distribution in the interval \\([0,1)\\). In Python, numpy.random provides the random() function, which does exactly this. A variety of other probability distributions are provided by numpy.random, and in general these should be used when possible. However, the techniques described here allow any desired PDF to be sampled.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "monte-carlo.html#pseudo-random-number-sampling",
    "href": "monte-carlo.html#pseudo-random-number-sampling",
    "title": "4  Monte Carlo Methods",
    "section": "",
    "text": "4.1.1 Uniform Probability Distributions\nWe will often need uniform probability distributions over an interval other than \\([0,1)\\). It is straightforward to map this interval to the desired one, as shown in the example below.\n\nimport numpy.random as random\nimport matplotlib.pyplot as plt\n\n# produce random numbers in the range 150-250\na = 100*random.random(int(1e5))+150\nplt.hist(a)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.1.2 Analytical Method\nThe above transformation is a special case of the method described in this section. We can consider a random number generator that produces values x over the interval \\((x_1, x_2)\\) with probability \\(P(x)\\), which we wish to convert to values y on the interval \\((y_1, y_2)\\), with probability \\(P'(y)\\). To construct a transformation from a generated value \\(x_{in}\\) to an output value with the required distribution, \\(y_{out}\\), we require that the cumulative distributions are equal :\n\\[\\int_{x_0}^{x_{in}} P(x) dx = \\int_{y_0}^{y_{out}} P'(y) dy \\tag{4.1}\\]\nNote that for \\(x_{in} = x_2\\), \\(y_{out} = y_2\\) both integrals must equal 1.\nIf the LHS of the above equation is uniform on the interval \\([0,1)\\), then we have :\n\\[x_{in} = \\int_{y_0}^{y_{out}} P'(y) dy \\tag{4.2}\\]\nIf we then define the function \\(Q(y_{out})\\) such that :\n\\[Q(y_{out}) = x_{in} = \\int_{y_0}^{y_{out}} P'(y) dy \\tag{4.3}\\]\nThen the transformation we must apply to \\(x_{in}\\), in order to obtain \\(y_{out}\\), is simply the inverse function, ie :\n\\[y_{out} = Q^{-1}(x_{in}) \\tag{4.4}\\]\n\n\n\n4.1.3 Analytical Method Example\nIn this example, we will write a function to produce values \\(y\\) in the interval \\([0, \\pi)\\) with probability distribution proportional to \\(\\sin(y)\\).\nHere, the integral above becomes :\n\\[\n\\begin{aligned}\nQ(y_{out}) &= \\frac{1}{2}\\int_{0}^{y_{out}} \\sin(y) dy \\\\\n           &= -\\frac{1}{2}\\cos(y_{out}) + C\n\\end{aligned}\n\\tag{4.5}\\]\nNote the factor \\(\\frac{1}{2}\\) is required to ensure the integral from \\(0\\) to \\(\\pi\\) is equal to 1. We can determine the constant of integration by requiring that \\(Q(0) = 0\\), for \\(x_{in}=0\\) and \\(Q(\\pi)=1\\) for \\(x_{in}=1\\).\nWe then find that\n\\[Q(y_{out}) = -\\frac{1}{2}\\cos(y_{out}) + \\frac{1}{2} \\tag{4.6}\\]\nAnd our inverse transformation is :\n\\[y_{out} = Q^{-1}(x_{in}) = \\cos^{-1}(1-2x_{in})  \\tag{4.7}\\]\n\ndef randSinAna():\n    \"\"\"Generate a random theta between 0 and pi, with PDF sin(theta) using analytical method\"\"\"\n    x = random.random()\n    return np.arccos(1-2*x)\n\n\nimport numpy as np\n\n# generate 50,000 points using a list comprehension\nn1s = [randSinAna() for _ in range(50000)]\n\n# plot a histogram\nhist1, bins1, patches1 = plt.hist(n1s, bins=50, density=True, label=\"Analytic Method\")\n\n# plot a sin(theta) function for comparison\nbin_centres = (bins1[1:] + bins1[:-1])/2\nplt.plot(bin_centres, np.sin(bin_centres)/2, label=r'$sin(\\theta)$')\n\nplt.xlabel(r'$\\theta$')\nplt.ylabel(r'$P(\\theta)$')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nIt might be worth highlighing the method used here (and in examples below) to generate a large number of points. This is a list comprehension. The list is generated by calling the first argument in the square brackets (here randSinAna()) for every iteration of the for loop. The for loop uses the underscore instead of a loop variable, since no variable is required. But in other cases, one could use a standard for loop to generate values in a list, eg :\n\n[i for i in range(5)]\n\n[0, 1, 2, 3, 4]\n\n\n\n\n\n4.1.4 Accept/Reject Method\nFor some PDFs, the integral required by the previous method cannot be determined analytically. In such cases, the accept/reject method provides a simple alternative. This method involves three steps : 1. a random number, \\(y\\), is generated in the desired interval \\((y_1, y_2)\\) 2. a second random number, \\(p\\), is generated between 0 and the maximum value of \\(P'(y)\\) 3. if \\(p &lt; P'(y)\\) then \\(y\\) is returned, otherwise it is rejected and the process is repeated\nThis method is clearly less efficient than the analytical method, since two random numbers are generated for each number returned, and some fraction of these are rejected. However, it allows us to generate any arbitrary probability distribution.\n\n\n\n4.1.5 Accept/Reject Example\nHere we demonstrate the accept/reject method for the same example as above, to produce values \\(y\\) in the interval \\([0, \\pi)\\), with probability distribution proportional to \\(\\sin(y)\\).\n\ndef randSinAR():\n    \"\"\"Generate a random theta between 0 and pi, with PDF sin(theta) using accept/reject method\"\"\"\n    while True:\n        x = np.pi * np.random.random()\n        y = np.random.random()\n        if y &lt; np.sin(x):\n            return x\n        else:\n            continue\n\n\n# generate 50,000 points using a list comprehension\nn2s = [randSinAR() for _ in range(50000)]\n\n# plot a histogram from the analytic method\nhist1, bins1, patches1 = plt.hist(n1s, bins=50, density=True, label=\"Analytic Method\")\n\n# plot another histogram from the accept/reject method\nhist2, bins2, patches2 = plt.hist(n2s, bins=bins1, density=True, label=\"Accept/reject\")\n\n# and the sin(theta) function for comparison\nplt.plot(bin_centres, np.sin(bin_centres)/2, label=r'$sin(\\theta)$')\n\nplt.xlabel(r'$\\theta$')\nplt.ylabel(r'$P(\\theta)$')\nplt.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "monte-carlo.html#multivariate-sampling",
    "href": "monte-carlo.html#multivariate-sampling",
    "title": "4  Monte Carlo Methods",
    "section": "4.2 Multivariate Sampling",
    "text": "4.2 Multivariate Sampling\nBoth analytic and accept/reject methods can be used to generate distributions of more than one variable. This often requires a bit of thought - the two methods may be more or less suitable to particular problems.\nHere we illustrate the two approaches to generating uniformly distributed random numbers on the unit disc.\n\n4.2.1 Analytic Example\nAn analytic method for the unit disc problem needs to ensure that the density of points is constant over the disc, ie that \\(P(x,y) \\propto dA\\) for area element \\(dA\\). In polar coordinates, we can write this as :\n\\[P(x,y) \\propto dA = r dr d\\phi\\]\nSince we will start by generating values with uniform distributions (let’s say \\(u\\) and \\(v\\)), we want to obtain transformations \\((u,v) \\rightarrow (r, \\phi)\\) such that :\n\\[dA = r dr d\\phi= du dv\\]\nClearly these substitutions are sufficient :\n\\[du = r dr\\] \\[dv = d\\phi\\]\nClearly we can just generate \\(\\phi\\) with a uniform distribution. The function to produce \\(r\\) from uniformly distributed \\(u\\) is obtained by integration :\n\\[u = \\frac{1}{2}r^2\\]\nand\n\\[r = \\sqrt{2u}\\]\nHowever, this will produce a disc with incorrect area. The required area is \\(\\pi\\), so we can obtain the constant of integration by requiring \\(\\int dA = \\pi\\), which gives :\n\\[r = \\sqrt{u}\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef unitDiscAna():\n    phi = 2 * np.pi * np.random.random()\n    r = np.sqrt(np.random.random())\n    \n    # convert to cartesian coordinates\n    x = r * np.cos(phi)\n    y = r * np.sin(phi)\n    \n    return np.array([x, y])\n\nps = np.array([unitDiscAna() for _ in range(1000)])\n\nplt.axis('equal')\nplt.scatter(ps[:,0], ps[:,1], marker='.', c='r')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.2.2 Accept/Reject Example\nSuppose we want to randomly generate points \\((x,y)\\) within a unit disc. A simple approach is to generate uniform distributions of \\(x\\) and \\(y\\) separately, and then use an accept/reject method to remove any points not in the disc (ie. where \\(\\sqrt{x^2 + y^2} \\gt 1\\)). This is illustrated in the example below.\n\ndef unitDiscAR():\n    x = 2 * np.random.random() - 1\n    y = 2 * np.random.random() - 1\n    while np.sqrt(x**2+ y**2) &gt; 1:\n        x = 2 * np.random.random() - 1\n        y = 2 * np.random.random() - 1\n    return np.array([x, y])\n\nps = np.array([unitDiscAR() for _ in range(1000)])\n\nplt.axis('equal')\nplt.scatter(ps[:,0], ps[:,1], marker='.', c='r')\nplt.show()\n\n\n\n\n\n\n\n\nNote that since the unitDisc() method returns a vector, we need to :\n\nconvert the list generated by the list comprehension (which calls unitDisc() many times) into a 2D array\nuse array slicing to obtain arrays of \\(x\\) and \\(y\\) values separately when plotting, ie. ps[:,0] gives a 1D array of \\(x\\) values.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "intro-ml.html",
    "href": "intro-ml.html",
    "title": "5  Introduction to Machine Learning",
    "section": "",
    "text": "5.1 What is Machine Learning ?\nMachine learning refers to algorithms that can learn patterns, or relationships, from data, and make predictions or decisions, without being explicitly programmed to do so. When applied to physics, this can mean that such algorithms infer structure (e.g. ‘the laws of physics’) in data without them having been encoded within the algorithm. This should be contrasted with a traditional approach to scientific computing where models are usually built from first principles.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "intro-ml.html#models",
    "href": "intro-ml.html#models",
    "title": "5  Introduction to Machine Learning",
    "section": "5.2 Models",
    "text": "5.2 Models\nMachine learning can refer a wide range of techniques. All include what we will refer to as a ‘model’, that is the basis for learning. The model, \\(f\\), transforms input data \\(x\\) into output \\(\\hat{y}\\). The model relies on some parameters (often referred to as ‘weights’), \\(\\theta\\), which are optimised as the model ‘learns’ from data.\n\\[\n\\hat{y}=f(\\theta, x)\n\\]\n(Here we use \\(\\hat{y}\\) to distinguish the model output from the ‘truth’ \\(y\\), although this distinction is not always required, as we shall see).\nThe model can take many forms, some of which you will have already encountered. A few are described below.\n\n5.2.1 Simple Function\nLinear regression is an example of supervised learning. The model here is the functional form that is being fitted to the data. Training is the process of determining parameters \\(\\theta\\) that minimise the mean squared error, i.e. performing the fit.\n\n\n5.2.2 Decision Trees\nDecision trees are commonly used in classification problems, in particular where the data occupies a multidimensional space. The data sample \\(x\\) is divided into categories \\(\\hat{y}\\) following a sequence of individual divisions or splits, which are defined by the parameters \\(\\theta\\). The precise sequence of divisions can be obtained using supervised learning, or clustering methods.\n\n\n5.2.3 Neural Networks\nNeural networks will be covered in detail in Chapter 7. Briefly, they comprise a network of nodes, modelled on the human brain. Each node receives input from several other nodes, and uses a activation function to generate output. Each input to the node is associated with a weight and a bias, which control the strength of the connection between nodes. The weights and biases are the parameters \\(\\theta\\) of the model, which are optimised during training. For example, the output of a node may be expressed as :\n\\[\ny = \\sum_{i}{a_i f(x_i + c_i)}\n\\]\nwhere the input data from input node \\(i\\) is \\(x_i\\), associated with weight \\(a_i\\) and bias \\(c_i\\), and the activation function is \\(f\\). Note that the activation function is typically non-linear, which allows the neural network to express complex non-linear transformations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "intro-ml.html#types-of-learning",
    "href": "intro-ml.html#types-of-learning",
    "title": "5  Introduction to Machine Learning",
    "section": "5.3 Types of learning",
    "text": "5.3 Types of learning\nThere are three main paradigms through which a machine learning model can ‘learn’. This process is frequently referred to as training. In this unit we will mainly focus on supervised learning, but we briefly describe all three paradigms here for completeness.\nEach method relies on some measure of the quality of the model output. This measure is frequently termed the loss, and is signed such that lower values of loss indicate improved output. During training, model parameters will be repeatedly updated, and the loss will be re-calculated, in order to find the set of parameters that minimise the loss.\n\n5.3.1 Supervised learning\nIn this case, the algorithm learns a mapping from its input data \\(x\\), to its output data \\(y\\), from a dedicated dataset. This dataset must be ‘labelled’, ie. it contains both \\(x\\) and corresponding true values \\(y\\). The algorithm is then applied to previously unseen data, which contains only \\(x\\), such that the algorithm makes a prediction, \\(\\hat{y}\\).\nAn example is the facial recognition technology found in many smartphones. Here the input data is an image from the phone camera, and the output data is a binary classifier with two values :\n\nOwner’s face\nNot owner’s face\n\nThe algorithm is trained during setup of the phone, while the camera image is known to contain the user’s face. Behind the scenes, the algorithm is also trained on many other images (both of other people’s faces, and not containing faces). During training, every image is labelled with the correct binary classifier, allowing the loss to be calculated. Later, when the phone is in use, the algorithm receives an input image from the camera, and will return a binary classifier indicating whether the image contains the owner’s face, or not.\nSupervised learning is frequently used in classification problems such as this example. It is also frequently used in regression problems, in which the algorithm output can be a number (or higher-dimensioned quantity - a vector or tensor). In this case, the loss is a measure of the difference between the prediction \\(\\hat{y}\\) and the truth \\(y\\).\n\n\n5.3.2 Unsupervised learning\nIn unsupervised learning, the goal is to find structures or patterns in the data \\(x\\), in the absence of ground truth \\(y\\). Unsupervised learning techniques include clustering and dimensionality reduction methods.\nIn some cases, the model is intended to describe the data \\(x\\) and may be capable of making predictions of new data \\(\\hat{x}\\). Here the loss will be a measure of compatibility between the prediction and the real data.\n\n\n5.3.3 Reinforcement learning\nIn reinforcement learning, the model is used by a software agent. Such an agent is a continuously running program which can make decisions or take action, within an environment. The environment provides feedback to the agent in response to each action/decision, which can be viewed as a reward/penalty. Training then involves updating model parameters to maximise the reward. An example is a robot navigating a maze, with the response from the environment being detection of collisions with the maze walls. When trained, the robot should then be capable of navigating the maze without hitting the walls.\n\n\n5.3.4 Other types of learning\nDeep learning is not strictly a mode of training. It is used to describe neural networks with many hidden layers, and can be applieed in any of the training paradigms described above.\nSemi-supervised learning is a mode which relies on both supervised and unsupervised learning. It is used in many advanced machine learning methods, and can reduce the computational requirements of training.\nTransfer learning is a method whereby a model trained on one dataset is subsequently trained on another. The result of the initial training (or pre-training) may be to learn some general feature identification, before the specific features of the target dataset are learnt. This method can result in reduced training times, in particular when pre-trained models are made widely available.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "intro-ml.html#challenges",
    "href": "intro-ml.html#challenges",
    "title": "5  Introduction to Machine Learning",
    "section": "5.4 Challenges",
    "text": "5.4 Challenges\nMachine learning offers benefits in the physical science, in terms of identifying patterns in data and exposing structure that cannot be found with traditional methods. However, it also raises several challenges, which are outlined briefly below. We will later explore some techniques which can be used to minimise these issues.\n\n5.4.1 Interpretability\nMachine learning is often capable of making excellent predictions in relation to complex dataset. However, as scientists we typically seek to explain nature, as well as make predictions. With traditional methods, models are often built from fundamental laws or relations, and the mathematics of the model then provide an explanation for the phenomena being studied. But machine learning models are frequently complex non-linear systems, from which it is difficult to extract explanations about why the predictions are correct.\n\n\n5.4.2 Overfitting\nThe goal of machine learning is usually to extract general patterns and structure in the data, which can be extrapolated to new situations. However, the model has no way to distinguish general features from noise in the data. Overfitting occurs when the model learns features (usually noise) that are specific to the precise dataset used during training, and is not generalisable.\n\n\n5.4.3 Uncertainty\nAs scientists we generally want to associate an uncertainty with any prediction or measurement. With traditional models we may use error propagation techniques to estimate uncertainties. However, machine learning models typically produce a single value, and due to their complex internal structure, error propagation is non-trivial.\n\n\n5.4.4 Reproducibility\nMachine learning can suffer from problems with reproducibility. Training typically relies on randomisation, both when updating weights, and also frequently in the order in which training data is presented. Given the computational requirements, training also typically uses parallel programming techniques, which can introduce small variations in results. These all contribute towards non-reproducibility in training.\n\n\n5.4.5 Sustainability\nRecent advances in machine learning and artificial intelligence have been made possible by huge increases in computing power. Large language models such as the GPT series have upwards of \\(10\\times10^{11}\\) parameters. Training a model such as GPT-3 (with \\(175\\times10^9\\) parameters) requires extremely large number of operations, estimated at \\(10^{23}\\) floating point operations. Using the Isambard AI facility, this would correspond to 25~GJ. Given the need to repeatedly train such a model to ascertain the optimal neural network configuation (or hyperparameters - not to be confused with the model weights themselves), the energy requirements evidently present a challenge for sustainability.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "intro-ml.html#examples-in-physics",
    "href": "intro-ml.html#examples-in-physics",
    "title": "5  Introduction to Machine Learning",
    "section": "5.5 Examples in Physics",
    "text": "5.5 Examples in Physics\nMachine learning has resulted in powerful applications in multiple areas of physics. A few examples are listed in the table below. The references given indicate key applications of deep learning within the field.\n\n\n\nArea\nExample Use\n\n\n\n\nHigh-energy physics\nEvent classification in particle detectors (Cowan et al, 2014)\n\n\nAstrophysics\nGalaxy morphology classification (Walmsley et al, 2019), transient detection\n\n\nCondensed matter\nDetecting phase transitions in spin models (Carrasquilla & Melko, 2017)\n\n\nQuantum physics\nNeural-network quantum states (Carleo & Troyer, 2017)\n\n\nPlasma physics\nML-based control of magnetic confinement (Tracey et al, 2022)\n\n\nComputational materials science\nSurrogate models for interatomic potentials (Anstine & Isayev, 2023)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html",
    "href": "supervised-ml.html",
    "title": "6  Supervised Learning",
    "section": "",
    "text": "6.1 Introduction\nIn computational physics, we often seek to build models that can learn from data. Supervised learning is one of the foundational paradigms in machine learning, in which a model learns to predict an output (label or target) given a set of inputs (features). In essence a model is trained to be able to identify patterns in datasets. If the model is trained successfully it should be able to predict outputs on unseen data.\nThis part of the course explores how supervised learning is formulated, trained, and evaluated, focusing on applications relevant to physics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#introduction",
    "href": "supervised-ml.html#introduction",
    "title": "6  Supervised Learning",
    "section": "",
    "text": "6.1.1 Training Sets\nIn supervised learning, the foundation is the training set, a collection of input–output pairs:\n\\[\n\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N\n\\]\nwhere:\n\\[\n(\\mathbf{x}_i \\in \\mathbb{R}^d)\n\\]\nare feature vectors (inputs), \\[\n(y_i \\in \\mathbb{R})\n\\] are labels (outputs), and\n\\[\n(N)\n\\] is the total number of samples.\nThe key goal of supervised learning is to learn a mapping: \\[\n(f: \\mathbf{x} \\rightarrow y)  \\text{such that}  (f(\\mathbf{x}_i) \\approx y_i).\n\\]\nThe significant point here is that one needs to have a labelled dataset to use supervised learning; each data point needs a corresponding label - a common example of this would be vehicles and vehicle types. The dataset would need to include different types of vehicle, such as motorbikes, cars, vans, lorries - and then each vehicle in the dataset would have a label identifiying its type. The different types of dataset used in supervised learning are covered in the next section.\n\n\n6.1.2 Dataset splitting\nGenerally there are three different types of data that are used in supervised learning. They can all come from the same dataset, but need to be split up in advance as they are used for different parts of the process.\nThe dataset is usually divided into:\n\nTraining set — used to fit the model parameters and learn the underlying patterns in the data.\nValidation set — used for tuning the hyperparameters of the model.\nTest set — used to evaluate final model performance - should be unbiased.\n\nTypical split ratios: 70% / 15% / 15%, though this does depend on how large the dataset is and the type of problem you are solving. There are techniques that one can use to augment data in the case of smaller datasets - these will be discussed later in the course.\n\n\n\n6.1.3 Models\nA model defines the relationship between inputs and outputs. It can be linear or non-linear, depending on the task the network has to perform and the relationship between the features and labels.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#linear-models",
    "href": "supervised-ml.html#linear-models",
    "title": "6  Supervised Learning",
    "section": "6.2 2.1 Linear Models",
    "text": "6.2 2.1 Linear Models\nA simple linear model assumes the relationship between the features and labels in a dataset can be accurately modelled by a straight line. There are several examples where we expect this to be the case; house prices based on size, exam scores based on number of hours studied…\nConsider the following dataset:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#nonlinear-models",
    "href": "supervised-ml.html#nonlinear-models",
    "title": "6  Supervised Learning",
    "section": "6.3 2.2 Nonlinear Models",
    "text": "6.3 2.2 Nonlinear Models",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#regression-losses",
    "href": "supervised-ml.html#regression-losses",
    "title": "6  Supervised Learning",
    "section": "7.1 3.1 Regression Losses",
    "text": "7.1 3.1 Regression Losses\n\nMean Squared Error (MSE): [ L = _i (y_i - f(_i))^2 ]\nMean Absolute Error (MAE): [ L = _i |y_i - f(_i)| ]\n\nMSE penalises large deviations more strongly, while MAE is more robust to outliers.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#classification-losses",
    "href": "supervised-ml.html#classification-losses",
    "title": "6  Supervised Learning",
    "section": "7.2 3.2 Classification Losses",
    "text": "7.2 3.2 Classification Losses\nFor binary or multi-class classification tasks, we often use: - Cross-Entropy Loss: [ L = - i c y{ic} p{ic} ]\nwhere (p_{ic}) is the predicted probability of class (c) for sample (i).\n\n7.2.1 Physics Example: Phase Classification\nGiven simulated data of a material at different temperatures and pressures, one might train a classifier to predict the phase (solid, liquid, gas) from thermodynamic observables. The cross-entropy loss measures how confidently and accurately the model predicts the correct phase.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#gradient-descent",
    "href": "supervised-ml.html#gradient-descent",
    "title": "6  Supervised Learning",
    "section": "8.1 4.1 Gradient Descent",
    "text": "8.1 4.1 Gradient Descent\nThe most common optimisation strategy is gradient descent, which updates parameters iteratively:\n[ _{t+1} = t - L(_t) ]\nwhere () is the learning rate.\n\n8.1.1 Variants\n\nStochastic Gradient Descent (SGD): uses random subsets (mini-batches) of data for faster convergence.\nMomentum / Adam / RMSProp: adaptive optimisers that improve convergence speed and stability.\n\n\n\n8.1.2 Example\nIn a neural network predicting potential energy surfaces, the parameters (weights) are adjusted using backpropagation—a computationally efficient way to apply the chain rule for gradients across many layers.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#validation",
    "href": "supervised-ml.html#validation",
    "title": "6  Supervised Learning",
    "section": "9.1 5.1 Validation",
    "text": "9.1 5.1 Validation\nThe validation set provides a means to monitor generalisation during training. We compute the validation loss:\n[ L_ = _{i } L(y_i, f(_i)) ]\nA rise in validation loss while training loss decreases indicates overfitting.\n\n9.1.1 Regularisation Techniques\nTo mitigate overfitting: - L2 regularisation (Ridge): adds (||^2) to the loss. - Dropout (for neural networks): randomly deactivates neurons during training. - Early stopping: halts training when validation loss stops improving.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#cross-validation",
    "href": "supervised-ml.html#cross-validation",
    "title": "6  Supervised Learning",
    "section": "9.2 5.2 Cross-Validation",
    "text": "9.2 5.2 Cross-Validation\nFor smaller datasets, k-fold cross-validation provides a robust estimate of model performance. Data are split into (k) folds, each serving once as validation while the others are used for training.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#regression",
    "href": "supervised-ml.html#regression",
    "title": "6  Supervised Learning",
    "section": "11.1 Regression",
    "text": "11.1 Regression\nLinear regression\nMinimisation of mean squared error\nConnection to maximum likelihood estimation under Gaussian noise\nPolynomial regression and basis expansion\nRegularisation (L2, L1)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#classification",
    "href": "supervised-ml.html#classification",
    "title": "6  Supervised Learning",
    "section": "11.2 Classification",
    "text": "11.2 Classification\nLogistic regression\nNegative log-likelihood loss (cross-entropy)\nConnection to MLE under Bernoulli model\nAlternatives : k-Nearest Neighbours (KNN). Support Vector Machines (SVMs)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#evaluating-model-performance",
    "href": "supervised-ml.html#evaluating-model-performance",
    "title": "6  Supervised Learning",
    "section": "11.3 Evaluating model performance",
    "text": "11.3 Evaluating model performance\nRegression metrics. MSE, RMSE, \\(R^2\\)\nClassification metrics. accuracy, precision, recall, F1 score, ROC curves.\nTrain/test split. Cross-validation.\nDemonstrate visually how overfitting and underfitting appear in training vs. validation error plots.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised-ml.html#bias-variance-overfitting",
    "href": "supervised-ml.html#bias-variance-overfitting",
    "title": "6  Supervised Learning",
    "section": "11.4 Bias, Variance, Overfitting",
    "text": "11.4 Bias, Variance, Overfitting\nDefine bias–variance trade-off.\nOverfitting in physics context: model fits noise rather than real phenomena.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "neural-networks.html",
    "href": "neural-networks.html",
    "title": "7  Neural Networks",
    "section": "",
    "text": "7.1 Anatomy of a Neural Network\nAn artifical neural network (ANN) is an interconnected collection of nodes or neurons, inspired by the human brain. Each neuron can receive input from many other neurons, performs some relatively simple processing, and then sends its output to one or many other neurons. Through careful choice of the processing performed by each neuron, the topology of the network, and advanced optimisation methods, the ANN can be trained to perform highly sophisticated processing tasks.\n## A Brief History of Artificial Neural Networks\nThe ANN was invented in 1943 by McCulloch & Pitts  [1]. The first implementation of an ANN was the Perceptron, by Rosenblatt in 1957, which was soon followed by the construction of a dedicated computer, the Mark 1 Perceptron, which ran a 3 layer ANN. These ANNs dealt with binary signals between neurons, and could not be trained. However, the first learning algorithm was developed in 1960  [adaline?], a single layer ANN that could be trained to minimise the mean squared error of its output. The first multi-layer perceptron followed in 1967  [shunichi-67?]. Backpropagation was invented in 1970 by Linnainmaa and applied to MLPs by Werbos. However, little research in this area followed until the 1990s.\nEarly applications of MLPs in physics.\nDeep learning 2010s.\nLLMs 2020s.\nMost ANNs are known as ‘feed-forward’ neural networks. This means the computation flows in one direction, from inputs to outputs. The neurons in an ANN are typically arranged in layers. The first layer receives the input data, and the final layer produces the output data. The dimensionality of the input and output layers will be determined by the shape of the input and output data. Typically there are a number of hidden layers between the input and output layers, for which the dimensionality is much less constrained - for example, the number of nodes can be greater than that of the input and output layers.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#anatomy-of-a-neural-network",
    "href": "neural-networks.html#anatomy-of-a-neural-network",
    "title": "7  Neural Networks",
    "section": "",
    "text": "An example of an Artifical Neural Network\n\n\n\n\n7.1.1 Neurons\nEach neuron processes input data according to a relatively simple function. The most simple neuron comprises a linear function of the inputs, followed by an activation function. The output can be written :\n\\[\ny = f\\left(\\sum_{i=0}^{n}{\\alpha_i x_i + \\beta_i} \\right)\n\\]\nwhere \\(\\alpha_i\\) are known as the weights, \\(\\beta_i\\) are the biases, \\(x_i\\) are the inputs to the neuron, and \\(y\\) is the neuron output. The function \\(f()\\) is known as the activation function.\nIn more advanced ANNs, neurons may different functions. For example, the sum over inputs may be replaced by another function.\n\n\n7.1.2 Connections\nThe connectivity between layers must be defined. Fully interconnected layers (such as the hidden layers in the diagram above) offer the most flexibility, since the weights associated with connections that are not required will tend to zero during training. However, the computational cost of such ‘dense layers’ may be excessive. Depending on the application, some layers in the ANN will not be fully connected. In image processing, for example, the input layers typically contain connections between nodes that corresponds to adjacent, or nearby, pixels.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#activation",
    "href": "neural-networks.html#activation",
    "title": "7  Neural Networks",
    "section": "7.2 Activation",
    "text": "7.2 Activation\nThe choice of activation function is important. If the activation function is a linear function, then the neural network is reduced to linear regression, i.e. the output of the ANN is a linear function of the ANN inputs. To capture more sophisticated features, it is important that the activation function provides some non-linearity.\nSome typical activation functions include :\n\nTanh function\nSigmoid function\nRectified linear function (ReLU)\n\nwhich are described in more detail below.\n\n7.2.1 tanh function\nThe hyperbolic tan function is defined as : \\[\ny = {\\rm tanh}(x) = \\frac{e^{2x}-1}{e^{2x}+1}\n\\]\nAnd plotted in the figure below.\n\n\n\n\n\n\n\n\n\nThis function maps an input range of \\([-\\infty, \\infty]\\) onto an output range of \\([-1, 1]\\). This ensures values do not grow unmanageably large as data propagates through the network, independent of weight magnitude. A linear combination of such functions can approximate a wide range of functions.\n\n\n7.2.2 Sigmoid function\nThe sigmoid function is another name for the logistic function.\n\\[\ny=\\frac{1}{1+e^{-x}}\n\\]\n\n\n\n\n\n\n\n\n\nThis function maps inputs on \\([-\\infty, \\infty]\\) onto outputs on \\([0, 1]\\). For this reason, the sigmoid is often chosen for classification problems, where the function value is treated as a probability of the data belonging to a given class. (See section on logistic regression).\n\n\n7.2.3 Rectified Linear (ReLU) function\nThe ReLU function is :\n\\[\n\\begin{align}\ny = \\, &x \\quad (x \\gt 0) \\\\\\\\\n&0 \\quad(x \\le 0)\n\\end{align}\n\\]\nOr, equivalently,\n\\[\ny = \\rm{ max}(x, 0)\n\\]\nThe ReLU function maps \\([-\\infty, \\infty]\\) onto \\([0, \\infty]\\). It is easily computed, and does not suffer from the “vanishing gradient problem” (see below).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#forward-pass",
    "href": "neural-networks.html#forward-pass",
    "title": "7  Neural Networks",
    "section": "7.3 Forward Pass",
    "text": "7.3 Forward Pass\nIn a feed-forward neural network, the input data is fed into the input layer, and the output of each layer of neurons is computed in turn until the output of the final layer is obtained. This output is treated as a prediction.\nThis allows the loss to be computed. For supervised learning, the loss is trypically the mean squared error (for regression problems) or the cross-entropy (for classification problems).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#backpropagation",
    "href": "neural-networks.html#backpropagation",
    "title": "7  Neural Networks",
    "section": "7.4 Backpropagation",
    "text": "7.4 Backpropagation\nIn order to train a neural network, the gradient of the loss with respect to the weights must be computed. This relies on back-propagation, a method by which the loss is propagated backwards through the network, such that the gradient with respect to each weight can be computed.\nUse chain rule to compute gradients of loss w.r.t. weights.\nIntuitive derivation, no need for full algebraic proof.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#training",
    "href": "neural-networks.html#training",
    "title": "7  Neural Networks",
    "section": "7.5 Training",
    "text": "7.5 Training\nBatch vs stochastic gradient descent Loss evolution: training loss vs. validation loss → early stopping to prevent overfitting.\nHyperparameters: learning rate, network depth, regularization, batch size.\nRegularization techniques:\nL2 weight decay\nDropout\nEarly stopping",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#deep-learning",
    "href": "neural-networks.html#deep-learning",
    "title": "7  Neural Networks",
    "section": "7.6 Deep Learning",
    "text": "7.6 Deep Learning\nMultiple hidden layers allow hierarchical feature extraction. Early layers learn simple features (edges, patterns). Deeper layers combine them into complex representations. Featurisation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "neural-networks.html#programming-neural-networks",
    "href": "neural-networks.html#programming-neural-networks",
    "title": "7  Neural Networks",
    "section": "7.7 Programming Neural Networks",
    "text": "7.7 Programming Neural Networks\nThere are several libraries in common use for neural network development, including PyTorch, Tensorflow, Keras, JAX. In this unit we will focus on PyTorch, which is an open source library originally developed at EPFL Lausanne, and subsequently by Facebook/Meta. Since 2022 it has been managed by the Linux Foundation.\n\n\n\n\n[1] P. McCulloch W. S., Bulletin of Mathematical Biophysics 5, 115 (1943).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] P. McCulloch W. S., Bulletin\nof Mathematical Biophysics 5, 115 (1943).",
    "crumbs": [
      "References"
    ]
  }
]